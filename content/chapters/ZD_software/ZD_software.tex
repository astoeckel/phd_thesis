% !TeX spellcheck = en_GB

\chapter{Software}
\label{app:software}

\begin{figure}[b!]
\begin{shaded}
\small%
\sffamily%
\setlength{\parskip}{0.5em}
\noindent\textbf{Software availability}\\[0.25cm]
Most of the software required to reproduce the experiments in this document is distributed as part of the thesis source code repository, see \url{https://github.com/astoeckel/phd_thesis}.

Operating system images facilitating the execution this code are available at \url{https://osf.io/y64xu/}.
Recent versions of \emph{libnlif} and NengoBio can be found at \url{https://github.com/astoeckel/libnlif} and \url{https://github.com/astoeckel/nengo-bio}, respectively.
\end{shaded}
\end{figure}

\begin{figure}[b!]
\begin{shaded}
\small%
\sffamily%
\noindent\textbf{Authorship and licensing}\\[0.25cm]
The software discussed in this chapter was primarily developed by the author.
Exceptions include dependencies such as \emph{eigen} and \emph{osqp}, as well as some smaller code snippets.
Authorship is explicitly noted in the individual files and directories.
Where applicable, the author's code is made available under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
Please visit \url{https://www.gnu.org/licenses/} to obtain a copy of the license.
\end{shaded}
\end{figure}

We developed several software libraries to conduct the research in this thesis, most notably \emph{libnlif} and \emph{NengoBio}.
Our weight and parameter solvers, as well as simulators for the \nlif neurons discussed in \Cref{chp:nlif} are implemented in \emph{libnlif}.%
\footnote{
Note that \emph{libnlif} supersedes \emph{libbioneuronqp} in a mostly backwards compatible way.
For legacy reasons, the source code of the experiments in \Cref{sec:two_comp_lif} still relies on \emph{libbioneuronqp}, as well as an older version of the code that eventually became \emph{libnlif} (initially published as supplementary material for \cite{stoeckel2021}).
}
The second library, NengoBio is an extension of the Nengo neural network simulation package \citep{bekolay2014nengo}.
Specifically, NengoBio implements the extensions to the NEF outlined in \Cref{sec:nef_extension} and interfaces with \emph{libnlif} for two compartment weight solving and simulation.
We use NengoBio as a part of our cerebellum model (cf.~\Cref{chp:cerebellum} and \cite{stockel2021connecting}).

Although these libraries are less interesting from a scientific perspective, we would still like to provide a quick overview of the software architecture and give some usage examples.

\section{An $n$-LIF Weight Solver and Simulator: libnlif}
\label{app:libnlif}

\emph{libnlif} is a hybrid Python and C++ library that facilitates working with \nlif neurons, specifically implementing the techniques discussed in \Cref{sec:nlif} onward.
This includes providing a simple API for describing and simulating \nlif neurons, as well as an implementation of the various synaptic weight and parameter optimisers from \Cref{sec:two_comp_lif,sec:nlif_opt}.

\subsection{Describing $n$-LIF Neurons}

The user-facing API for describing neuron models is heavily inspired by Nengo.
For example, a standard LIF neuron with current-based input (cf.~\Cref{fig:nlif_a}) can be constructed as follows:
\begin{pythoncode}
import nlif

with nlif.Neuron() as lif: # Create a new neuron description 
	with nlif.Soma(v_th=-50e-3, tau_ref=2e-3, tau_spike=1e-3, C_m=1e-9) as soma:
		gL = nlif.CondChan(g=50e-9, E_rev=-80e-3) # Static leak channel
		J = nlif.CurChan(mul=1.0)                 # Current-based input channel

lif_assm = two_comp_lif.assemble() # Assemble an immutable representation
\end{pythoncode}
Similarly, the following code describes the two-compartment LIF neuron (cf.~\Cref{fig:nlif_c}):
\begin{pythoncode}
with nlif.Neuron() as two_comp_lif:
	with nlif.Soma(v_th=-50e-3, tau_ref=2e-3, tau_spike=1e-3, C_m=1e-9) as soma:
		gL = nlif.CondChan(g=50e-9, E_rev=-65e-3)  # Static leak channel
	with nlif.Compartment(C_m=1e-9) as dendrites:
		gL = nlif.CondChan(g=50e-9, E_rev=-65e-3)  # Static leak channel
		gE = nlif.CondChan(E_rev=0e-3)             # Excitatory input channel
		gI = nlif.CondChan(E_rev=-75e-3)           # Inhibitory input channel
	nlif.Connection(soma, dendrites, g_c=50e-9)
two_comp_lif_assm = two_comp_lif.assemble()
\end{pythoncode}
This overall pattern extends to arbitrary connectivity graphs.
Individual objects and the graph structure are validated for adherence to the \nlif constraints (\Cref{sec:nlif_description}) in the \texttt{assemble} method.
The Python \enquote{with} statement is used to establish an object hierarchy; this is accomplished by overriding the \texttt{\_\_enter\_\_} and \texttt{\_\_exit\_\_} functions and tracking the current parent object in a thread-local stack.
Objects are automatically labelled according to their local variable name whenever a \enquote{with}-scope is left.

Calling the \enquote{\texttt{to\_svg}} method on the assembled neuron object (or simply evaluating the object in a Jupyter cell) generates an annotated \enquote{ball-and-stick} representation of the neuron (similar to those depicted in~\Cref{fig:nlif}) using \emph{GraphViz} \citep{ellson2004graphviz}.

\subsection{Simulating $n$-LIF Neurons}

\begin{figure}
	\centering
	\includegraphics{media/chapters/ZD_software/libnlif_simulation_example.pdf}
	\label{fig:libnlif_simulation_example}
	\caption[Output of the \emph{libnlif} two-compartment LIF simulation code example]{Output of the \emph{libnlif} two-compartment LIF simulation code example. Output spikes are available as individual spike times at sub-sample resolution or as a discretised sum of Dirac deltas.}
\end{figure}

The assembled $n$-LIF object holds the system matrices \mnAp, \mnBp, \vnap, \vnbp, \mnL (\Cref{sec:nlif_description}).
These matrices can be accessed through the \texttt{A}, \texttt{B}, \texttt{a\_const}, \texttt{b\_const} and \texttt{L} properties and are used by \emph{libnlif} to simulate the dynamics of the neuron.
Specifically, the neural dynamics can be simulated using the \texttt{nlif.Simulator} object and one of the available \texttt{simulate} methods:
\begin{pythoncode}
dt, ss, T = 1e-4, 10, 1.0     # Simulation time-step, sub-sampling and end-time
ts = np.arange(0, T, ss * dt) # Sample points
gEs = np.linspace(0.0, 200e-9, len(ts)) # Sampled input conductances
with nlif.Simulator(two_comp_lif_assm, dt=dt, ss=ss,
                    record_voltages=True, record_spike_times=True) as sim:
	res = sim.simulate({      # Also: simulate_poisson, simulate_filtered
		gE: gEs,              # Sampled input
		gI: 10e-9})           # Constant input
plt.plot(ts, res.v)           # Discretised output pulses are stored in res.out
\end{pythoncode}
The output of this code is depicted in \Cref{fig:libnlif_simulation_example}.
Other variants of the \enquote{simulate} method include \texttt{simulate\_poisson}, which generates artificial Poisson-distributed spike noise \citep[e.g.,][Section~1.4]{abbott2001theoretical} and treats the given inputs as expectation values, as well as \texttt{simulate\_filtered}, which filters the input using a first-order low-pass.

\subsubsection{Automatic simulator generation and compilation}
Instantiating the \texttt{Simulator} class transparently generates a dynamically linked library that implements a specialised simulator.
The library is based on a templated \citep[Chapter~23]{stroustrup2013programming} C++ simulator (cf.~\texttt{simulator.hpp});
substituting in the specific system matrices and simulator options and compile-time maximises run-time performance.
The code is compiled using a custom C++ build system (see \texttt{cmodule.py}) that calls out to the GNU Compiler Collection\footnote{See \url{https://gcc.gnu.org/} for more information.} and caches the generated object files according to their input file hashes.
Reentrancy of the build system is ensured by employing an idempotency mechanism based on temporary target files and the atomic \enquote{rename} filesystem operation \citep[pp.~1816-1820]{2018ieee}.
The compiled library is loaded via the Python \texttt{ctypes} foreign function interface; the C++ code directly operates on the memory regions that back the Numpy arrays.


Internally, the simulator uses the Eigen linear algebra library \citep{eigenweb}.
The dynamical system is integrated under a zero-order hold assumption by solving for the system state at time $t + \Delta t$ according to the closed-form matrix exponential form \cref{eqn:nlif_dynamics}.

\subsubsection{Input sweeps}

\begin{figure}
	\centering
	\includegraphics{media/chapters/ZD_software/libnlif_rate_empirical_example.pdf}
	\caption[Output of the \emph{libnlif} \enquote{rate\_empirical} function for different noise parameters]{Output of the \emph{libnlif} two-compartment LIF \texttt{rate\_empirical} rate example. The specified $\lambda$ corresponds to the rate of the Poisson spike source; the low-pass filter time constant $\tau$ is set to $\SI{5}{\milli\second}$.}
	\label{fig:libnlif_rate_empirical_example}
\end{figure}

The assembled neuron object provides convenience functions for sampling the multi-variate response curves $\mathscr{G}(\vec g)$ (eq.~\ref{eqn:def_response_curve_g}).
The following code simulates the neuron for \SI{100}{\second} and estimates the average firing rate by taking the inverse of the median inter-spike interval.
The optional \texttt{noise}, \texttt{rate}, and \texttt{tau} parameters specify Poisson-distributed spike-noise.
\begin{pythoncode}
gEs, gIs = np.linspace(0, 1e-6, 20), np.linspace(0, 1e-6, 20) 
gEss, gIss = np.meshgrid(gEs, gIs)        # Generate a dense sample grid
rates = two_comp_lif_assm.rate_empirical( # Simulate the neuron at each sample
	{gE: gEss, gI: gIss}, T=100.0, noise=True, rate=10000, tau=5e-3)
\end{pythoncode}
Evaluation of the sample points is distributed across all available processor cores.
The output of the above code-snippet (with a $100 \times 100$ grid) is visualised in \Cref{fig:libnlif_rate_empirical_example} for different rates $\lambda$.

\subsection{Predicting Somatic Currents and Solving For Parameters and Weights}

\begin{figure}
	\centering
	\includegraphics{media/chapters/ZD_software/libnlif_parameter_estimation.pdf}%
	{\phantomsubcaption\label{fig:libnlif_parameter_estimation_a}}%
	{\phantomsubcaption\label{fig:libnlif_parameter_estimation_b}}%
	{\phantomsubcaption\label{fig:libnlif_parameter_estimation_c}}%
	\caption[Two-compartment LIF parameter optimisation using \emph{libnlif}]{Two-compartment LIF parameter optimisation using \emph{libnlif}.
	Filled contours are the empirical error measurements, dashed white lines the predicted somatic currents.}
	\label{fig:libnlif_parameter_estimation}
\end{figure}

The assembled neuron object can also predict somatic currents according to \cref{eqn:h_model}.
To this end, we must first compute the reduced system matrices \mrAp, \mrBp, \vrap, \vrbp, \mrL, \vrc:
\begin{pythoncode}
sys = two_comp_lif_assm.reduced_system(v_som=None) # v_som = 0.5*(v_reset+v_th)
\end{pythoncode}
%Here, \texttt{v\_som=None} implies that the average somatic potential \vSom is computed as the mean between the reset and threshold potential.
The reduced system matrices are stored in the \texttt{A}, \texttt{B}, \texttt{a\_const}, \texttt{b\_const}, \texttt{L} and \texttt{c} properties of the \texttt{sys} object.
Currents can be predicted as follows:
\begin{pythoncode}
i_som_pred = two_comp_lif_assm.i_som({gE: gEss, gI: gIss}, reduced_system=sys)
\end{pythoncode}

\subsubsection{Estimating model parameters}
As depicted in \Cref{fig:libnlif_parameter_estimation_a}, \texttt{i\_som\_pred} is not very accurate without further calibration.
The \texttt{nlif.parameter\_optimisation} package implements the optimisation methods discussed in \Cref{sec:nlif_opt_parameters}, including our 
our soft trust-region based \SQP:
\begin{pythoncode}
gs_train = two_comp_lif_assm.canonicalise_input({gE: gEss, gI: gIss})
Js_train = two_comp_lif_assm.lif_rate_inv(rates) # Invert the response curve
valid = rates > 12.5                             # Discard subthreshold samples
sys = sys.condition()                            # Condition the reduced system
sys_opt, errs_train = nlif.parameter_optimisation.optimise_trust_region(
	sys, gs_train=gs_train[valid], Js_train=Js_train[valid], N_epochs=10)
\end{pythoncode}
The optimised system is stored in \texttt{sys\_opt} and can be passed to the above \texttt{i\_som} method to obtain improved current predictions (\Cref{fig:libnlif_parameter_estimation_b,fig:libnlif_parameter_estimation_c}).
Just like the \nlif simulator, the trust-region optimiser relies on a dynamically compiled C++ library that in turn makes use of the Eigen and OSQP libraries (\cite{stellato2020osqp}; see \texttt{nlif\_solver\_parameters.cpp}).

\subsubsection{Solving for synaptic weights}

\begin{figure}
	\includegraphics{media/chapters/ZD_software/libnlif_weight_solver.pdf}
	\caption[eight optimisation code for the two-compartment neuron and nonnegative multiplication]{Weight optimisation code for the two-compartment neuron and nonnegative multiplication. Coloured contours and dotted lines show the decoded currents; dashed lines are the target.}
\end{figure}

The synaptic weight solvers discussed in \Cref{sec:nlif_opt_weights} are provided in the \texttt{nlif.weight\_optimisation} package.
For example, to find synaptic weights that approximate non-negative multiplication we can use the following code:
\begin{pythoncode}
# Sample the represented space
xs, ys = np.linspace(-1, 1, 101), np.linspace(-1, 1, 101)
xss, yss = np.meshgrid(xs, ys)
Xs = np.array((xss.flatten(), yss.flatten())).T

# Obtain the target current function and pre-activities
Js = 0.5 * (1.0 + Xs[:, 0]) * (1.0 + Xs[:, 1]) * 1e-9
As = [...] # (N_smpls x m_pre); use the NEF tuning curve equations here

# Perform the actual training
idcs = np.random.randint(0, Xs.shape[0], 256)  # Training samples
W_mask = np.ones((2, As.shape[1]), dtype=bool) # All-to-all connectivity
W, errs_train = nlif.weight_optimisation.optimise_trust_region(sys_opt,
    As_train=As[idcs], Js_train=Js[idcs], W_mask=W_mask, N_epochs=10)

# Compute the decoded currents
Js_dec = two_comp_lif_assm.i_som(As @ W.T, reduced_system=sys_opt)
\end{pythoncode}
Again, the optimiser is implemented in a C++ library (see \texttt{nlif\_solver\_weights.cpp}).
Since the generated quadratic programs tend to be moderately large (on the order of thousands of variables, depending on the number of pre-neurons and samples), the code makes extensive use of sparse matrices.
To prevent heap reallocations while populating the matrices, the code pre-calculates the number of non-zero entries per column and sequentially writes the matrix coefficients to memory.
The low-level \texttt{nlif.Solver} class wraps the C++ code more directly and supports concurrent optimisation across multiple post-neurons.

\section{More Biologically Plausible Nengo Models: NengoBio}
\label{app:nengo_bio}

NengoBio is an add-on to the Nengo neural network simulator package \citep{bekolay2014nengo} that implements the extensions to the NEF discussed in \Cref{sec:nef_extension}.
At its core, NengoBio adds support for multi-channel neurons and for optimising weights in current-space.
We accomplish this by hooking into Nengo's build system and dynamically rewriting the operator graph (see \cite{gosmann2017automatic} for a description of the operator graph).

NengoBio can account for Dale's principle, provide special syntactic sugar for interneuron populations, enforce sparsity constraints, and support dendritic computation with two-compartment LIF neurons.
We discuss these features in more detail in the following subsections.

\subsection{Accounting for Dale's Principle}

Dale's principle can be enforced by replacing Nengo's original \texttt{Ensemble} and \texttt{Connection} objects with those provided by NengoBio and setting the \texttt{p\_exc} or \texttt{p\_inh} properties when constructing the pre-population.
The following code-snippet demonstrates this
\begin{pythoncode}
import numpy as np; import nengo; import nengo_bio as bio

with nengo.Network() as model:
	# Construct the input and two NengoBio ensembles
    input = nengo.Node(lambda t: np.sin(2.0 * np.pi * t))
    ens1 = bio.Ensemble(n_neurons=100, dimensions=1, p_exc=1.0)
    ens2 = bio.Ensemble(n_neurons=100, dimensions=1)

	# Nengo connections can target bio.Ensemble objects
    nengo.Connection(input, ens1)

	# NengoBio connections can only be between bio.Ensemble objects. bio.Decode
	# (default) forces decoding bias currents; bio.JBias implies intrinsic biases.
    bio.Connection(ens1, ens2, bias_mode=bio.Decode)
\end{pythoncode}
This is implemented using the NNLS method discussed in \Cref{sec:nef_nonneg}.
Values between zero and one for \texttt{p\_exc} sets the probability with which a neuron is marked as excitatory.
Not setting \texttt{p\_exc} disables Dale's principle for outgoing connections.

NengoBio hooks into the Nengo GUI visualiser%
\footnote{See \url{https://github.com/nengo/nengo_gui} for more details.}
to highlight purely excitatory or inhibitory connections; with some minor modifications (i.e., repeating the imports and \enquote{with} block), the code-snippets in this section can be copied into the visualiser for exploration.

\subsection{Inhibitory Interneuron Populations and Communication Channels}
\label{sec:nengo_bio_inhibitory}

In \Cref{sec:nef_nonneg}, we discussed two interesting network architectures that benefit from current-space weight solving: networks with inhibitory interneuron populations and purely inhibitory communication channels.
Both network types can be easily implemented in NengoBio.

\subsubsection{Inhibitory interneurons}
In biology, inhibitory input to a neuron population is often routed through inhibitory interneurons.
This results in the network architecture depicted in \Cref{fig:inhibitory_interneurons}.
We suggested constructing such networks by assuming that both the interneurons and the excitatory pre-population represent the same value $\vec x$.
NengoBio provides special syntax for this: specifying a set \texttt{\{ens1, \textellipsis, ensN\}} as a pre-population in a connection results in these ensembles being treated as a virtual pre-population that represents a common value:
\begin{pythoncode}
# Create the three ensembles
ens_exc = bio.Ensemble(n_neurons=100, dimensions=1, p_exc=1.0)
ens_inh = bio.Ensemble(n_neurons=100, dimensions=1, p_inh=1.0)
ens_tar = bio.Ensemble(n_neurons=100, dimensions=1)

# Setup connections, treat {ens_exc, ens_inh} as a single population
nengo.Connection(input, ens_exc)
bio.Connection(ens_exc, ens_inh)
bio.Connection({ens_exc, ens_inh}, ens_tar, function=lambda x: x**2)
\end{pythoncode}

\subsubsection{Purely inhibitory communication channels}
We can construct purely inhibitory communication channels---and even compute functions along these channels---as long as the target population receives \emph{some} other form of excitatory input.
As suggested in \Cref{eqn:inhibitory_communication_channel}, this can be accomplished by ignoring the excitatory pre-population in the represented space, but using the pre-activities to solve for excitatory input.
\begin{pythoncode}
# Random input for the excitatory ensemble; should not influence the computation
input_noise = nengo.Node(nengo.processes.WhiteSignal(high=5.0, period=10.0))

# Setup connections, treat (ens_inh, ens_exc) as a single population
nengo.Connection(input_noise, ens_exc)
nengo.Connection(input, ens_inh)
bio.Connection((ens_inh, ens_exc), ens_tar, function=lambda x: x[0]**2)
\end{pythoncode}
Note that we listed the two pre-populations as a tuple \texttt{(ens1, \textellipsis, ensN)}.
This instructs NengoBio to form a virtual pre-population where the represented values of the populations are stacked.
Here, the two pre-populations represent one-dimensional quantities---correspondingly, the post-population receives a two-dimensional value.
In the above code, we simply ignore the second dimension that originates from the excitatory pre-neuron.

\subsection{Sparsity Constraints}

\begin{figure}
	\centering
	\includegraphics{media/chapters/ZD_software/nengo_bio_spatial_connectivity.pdf}
	\caption[Spatial connectivity constraints in NengoBio]{
		Spatial connectivity constraints in NengoBio.
		\textbf{(A)} Location of each neuron and connections.
		\textbf{(B)} Normalised connection probabilities.
		\textbf{(C)} Testing the communication channel.
	}
	\label{fig:nengo_bio_spatial_connectivity}
\end{figure}

As we discussed in \Cref{chp:cerbellum}, neurobiological microcircuits are often characterised in terms of their convergence and divergence numbers.
NengoBio can account for these constraints.

\subsubsection{Specifying convergence and/or divergence}
Connectivity constraints can be specified by passing a \texttt{Connectivity} object to a connection.
For example, the following code-snippet establishes random connectivity that takes the given convergence numbers into account:
\begin{pythoncode}
ens_src = bio.Ensemble(n_neurons=100, dimensions=1)
ens_tar = bio.Ensemble(n_neurons=100, dimensions=1)
bio.Connection(ens_src, ens_tar, # Can alternatively pass "divergence" (or both)
               connectivity=bio.ConstrainedConnectivity(convergence=5))
\end{pythoncode}
%#It is possible to alternatively specify the divergence of the connection.
%When specifying both convergence and divergence NengoBio will treat the values as upper bounds.

\subsubsection{Spatially constrained connectivity}
NengoBio also supports spatially constrained connectivity.
Each ensemble can be assigned an array or distribution of locations in $n$-dimensional space; this location information is then used to compute connection probabilities:
\begin{pythoncode}
ens_src = bio.Ensemble(n_neurons=25, dimensions=1,
                       locations=bio.NeuralSheetDist(dimensions=2))
ens_tar = bio.Ensemble(n_neurons=100, dimensions=1,
                       locations=bio.NeuralSheetDist(dimensions=2))
bio.Connection(ens_src, ens_tar, connectivity=
               bio.SpatiallyConstrainedConnectivity(convergence=5, sigma=0.25))
\end{pythoncode}
Here, \texttt{NeuralSheetDist} is a random distribution that arranges neurons along a Hilbert curve to ensure approximate equidistance and that neurons with similar indices are close together in space.
The probability matrix and final connectivity of this network are depicted in \Cref{fig:nengo_bio_spatial_connectivity}.

\subsection{Dendritic Computation}
Dendritic computation in NengoBio relies on the tuple-syntax for specifying a virtual pre-population with stacked represented values (\Cref{sec:nengo_bio_inhibitory}).
As we discussed in detail in \Cref{chp:nlif}, we can only compute additive functions in the pre-populations with standard current-based LIF neurons.
In NengoBio, simply setting the neuron type of an ensemble to the two-compartment LIF neuron enables the computation of nonlinear multivariate functions:
\begin{pythoncode}
# Input nodes and pre-populations with 30% inhibitory neurons
inp_x1, inp_x2 = nengo.Node(size_in=1), nengo.Node(size_in=1)
ens_x1  = bio.Ensemble(n_neurons=100, dimensions=1, p_inh=0.3)
ens_x2  = bio.Ensemble(n_neurons=100, dimensions=1, p_inh=0.3)
nengo.Connection(inp_x1, ens_x1)
nengo.Connection(inp_x2, ens_x2)

# Create a population of two-compartment LIF neurons, use lower maximum rates
# because of the current limit imposed by the conductance-based synapses
ens_tar = bio.Ensemble(n_neurons=100, dimensions=1,
                       neuron_type=bio.neurons.TwoCompLIF(),
                       max_rates=nengo.dists.Uniform(75, 100))

# Compute nonnegative multiplication. Map the input from [-1, 1]^2 onto [0, 1]
# and the output from [0, 1] onto [-1, 1]. Use the quadratic programming solver
# with subthreshold relaxation to improve performance.
bio.Connection((ens_x1, ens_x2), ens_tar,
               function=lambda x: 0.5 * (x[0] + 1.0) * (x[1] + 1.0) - 1.0,
               solver=bio.solvers.QPSolver(relax=True))
\end{pythoncode}
When using the two-compartment LIF neuron, NengoBio automatically calibrates the two-compartment neuron parameters using the method presented in \Cref{sec:two_comp_lif_fit_model}.
In particular, NengoBio performs a series of neuron simulations to determine the excitatory and inhibitory conductance range that produces the specified maximum firing rates.

The \texttt{QPSolver} class uses the non-sequential quadratic program discussed in \Cref{sec:two_comp_synaptic_weights} to compute synaptic weights.
An implementation of this non-sequential weight solver is provided by \emph{libnlif} in addition to the sequential soft trust-region based algorithms.

As of writing, NengoBio does \emph{not} support \nlif-neurons with more than two compartments.
Given our implementation of the parameter and weight solver implementations in \emph{libnlif}, this is not a technical challenge per se, but instead hinges on finding a good API that allows users to specify possible synaptic sites.
For example, some pre-populations may only connect to basal compartments, whereas others only target apical compartments.
It is unclear how users would specify this at an abstract level.