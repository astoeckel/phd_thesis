% !TeX spellcheck = en_GB

So far we have mostly ignored network-level dynamics.
Clearly, our analysis of \emph{individual} \nlif neurons relied on harnessing the subthreshold dynamics of passive dendrites for computation.
However, we primarily focused on the equilibrium state of this system and accounted for dynamics by fitting a static model to simulation data.
We would certainly have reached lower errors in our experiments had the model not possessed additional dynamics.

Even though neural dynamics may appear unwieldy, they play a critical role in cognition.
More precisely, the central hypothesis of this chapter---and, if you will, a theoretical neuroscience \emph{theory} (cf.~\Cref{chp:modelling_neurobiological_systems})---is that temporal resources in biological neural networks give rise to the \emph{temporal tuning} of individual neurons.
These underlying temporal resources include synaptic filters, intrinsic neural dynamics, transmission delays, and the temporal tuning of neighbouring neurons.
Such resources give rise to diverse temporal tuning curves and synaptic weights recombine those to ultimately support high-level functions such as interpreting visual motion, forming working memory, or generating motor trajectories.
Mathematically speaking, we can decode higher-order functions $f$ that not only depend on the current represented value $\vec x \in \mathbb{R}^d$ alone, but on a \emph{signal} $\mathfrak{x}(t) : \mathbb{R}^- \longrightarrow \mathbb{R}^d$, where $t = 0$ corresponds to the present, and $t < 0$ references the past (cf.~\Cref{tbl:spatiotemporal}; we define $\mathbb{R}^- = \{ t \leq 0 \mid t \in \mathbb{R} \}$).

Notably, our temporal tuning curve approach naturally extends the \NEF dynamics principle \citep[Chapter~8]{eliasmith2003neural}.
That is, using temporal tuning curves, we can realise any dynamical system that could be realised using the dynamics principle.%
\footnote{While it is possible to realise nonlinear dynamical systems using temporal tuning curves, we focus on linear dynamical systems for the sake of simplicity.}
At the same time, temporal tuning curves facilitate taking the temporal properties of pre-populations and intermediate filters into account, including heterogeneous and higher-order synapses.

However, the most important contribution of our temporal tuning curve approach is a shift in perspective.
Just as we harness non-temporal tuning curves and neural nonlinearities to approximate spatial functions in neural networks, we are now able to harness the temporal tuning of pre-populations and intermediate filters to realise the temporal tuning of post-populations.
Curiously, this includes recurrent neuron populations.
We furthermore separate \emph{temporal} from \emph{spatial} tuning.
A neuron can be tuned to a $d$-dimensional spatial quantity, and, at the same time, possess some temporal tuning, typically of finite order $q$.

Crucially, this allows modellers to directly specify some desired neural behaviour.
For example, a modeller could choose to assign time-cell like tuning \citep{pastalkova2008internally,tiganj2016sequential} to one population, and visual spatiotemporal receptive fields \citep{carandini1999linearity} to another.
In theory, these tuning properties can be provided in the form of sampled data.
So far, and as has been criticised by \citep{nicola2017supervised}, this was not possible using the \NEF dynamics principle---while the representation and transformation principle facilitate incorporating empirical data, the dynamics principle relied on modellers providing a mathematical model in terms of differential equations, with uncertain consequences for the impact on neural tuning.

Almost more strikingly, temporal tuning is not just useful for modelling neurobiological systems.
Introducing the right kind of dynamics into \emph{artificial} neural networks results in architectures that outperform successful approaches in machine learning by wide margins.
Specifically, we show that a layer of Legendre Memory Units (\LMUpl; \cite{voelker2019lmu}) corresponds to a spatiotemporally tuned \NEF population. We use our insights from modelling biological systems to propose alternative variants of this system.

\subsubsection{Structure}
We start \Cref{sec:temporal_tuning_curves} with a review of examples of \enquote{temporal tuning} in biology, and extend the \NEF by introducing the concept of a \enquote{temporal tuning curve}.
Specifically, we propose a linear least-squares optimisation problem that realises the desired temporal tuning by taking synaptic filters and the pre-population tuning into account.
Under ideal conditions, temporal tuning curves can be used to construct recurrent neural networks that realise \LTI systems while compensating for heterogeneous and higher-order synaptic filters.

Given that we can tune neurons to arbitrary \LTI systems, we ask in \Cref{sec:temporal_bases} what \LTI system we should optimally implement to obtain a maximally diverse set of temporal tuning curves.
This leads us to the concept of \LTI systems approximating sliding-window spectra.
We introduce a simple technique for constructing such \LTI systems and re-derive the Legendre Delay Network (\LDN) using this approach.
Furthermore, we discover a \enquote{modified Fourier system} as a potential alternative to the \LDN.

In \Cref{sec:recurrent_weights}, we demonstrate that our temporal tuning curve approach still works in less ideal spiking neural networks.
We show that, just like the \LDN, our modified Fourier system generates \enquote{time cells} when mapped onto a spiking neural network.
Furthermore, we use spatiotemporal encoding matrices to generate multi-dimensional spatiotemporal tuning and demonstrate decoding nonlinear functions over both space and time.

Finally, in \Cref{sec:applications_to_ml}, we draw parallels between \LMU layers and our aforementioned \NEF populations with spatiotemporal tuning.
We show that the choice of the sliding window-spectrum is secondary to the performance of the \LMU and demonstrate that convolving input signals with the modified Fourier system can offer a significant improvement in terms of asymptotic time-complexity compared to the \LDN.
We close with a discussion \Cref{sec:temporal_tuning_conclusion}.

\begin{Notation}
We sometimes use Fraktur letters (e.g., $\mathfrak{a}$, $\mathfrak{b}$, \textellipsis) to convey that the mathematical objects we are referring to are now \emph{functions} over time.
That is, we write a temporal encoder as $\vec{\mathfrak{e}}$ instead of $\vec e$.
Note that $\vec{\mathfrak{e}}(t)$ refers to the result of evaluating the function $\vec{\mathfrak{e}}$ at a specific point in time $t$, whereas $\vec{\mathfrak{e}}$ refers to the function object as a whole.
In other words, $\vec{\mathfrak{e}}$ is a map $\mathbb{R} \longrightarrow \mathbb{R}^d$, whereas $\vec{\mathfrak{e}}(t)$ is an element in $\mathbb{R}^d$.
This distinction is important when expressing operations on functions, such as convolution.
We furthermore assume in most of our definitions that $t = 0$ corresponds to the current point in time.
%Since nervous systems typically do not have access to future events, the stimulus history $\vec{\mathfrak{x}}(t)$ only needs to be defined for $t \leq 0$.
\end{Notation}
