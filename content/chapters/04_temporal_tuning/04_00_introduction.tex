% !TeX spellcheck = en_GB

In the previous chapter, we exploited the dynamics of multi-compartment \nlif neurons to compute a larger class of functions than previously possible with a single layer of \LIF neurons.
However, we primarily focused on the equilibrium state of the \nlif system.
In other words, our dendritic nonlinearity $H$ did not expose the underlying dynamics to the network-level.
This can be problematic for two reasons.
First, when approximating dynamical systems using \NEF principle three (\Cref{sec:nef_dynamics}), not compensating for intrinsic neural dynamics may result in deviations from the desired outcome (cf.~\Cref{sec:nef_limitations}).
Second, the intrinsic dynamics could in theory help establish some desired dynamics; they may be viewed as a temporal resource that can be \emph{harnessed} for computation, and not just \emph{compensated} for.

The goal of this chapter is to extend the \NEF to more readily take a variety of temporal resources into account.
While we still focus on synaptic filters as a primary source of neural dynamics, the same approach can in principle accommodate intrinsic neural dynamics as well.
More precisely, the central hypothesis of this chapter---and, if you will, a theoretical neuroscience \emph{theory}---is that temporal resources in biological neural networks give rise to the \emph{temporal tuning} of individual neurons.
These underlying temporal resources include synaptic filters, intrinsic neural dynamics, transmission delays, and the temporal tuning of neighbouring neurons.
Such resources then establish diverse temporal tuning curves, and are recombined by synaptic weights to ultimately support high-level functions such as interpreting visual motion, forming working memory, or generating motor trajectories.
Mathematically speaking, we can decode higher-order functions $f$ from a temporally tuned neuron population that not only depends on the current represented value $\vec x \in \mathbb{R}^d$, but on a \emph{signal} $\mathfrak{x}(t)$ (cf.~\Cref{tbl:spatiotemporal}).

Notably, our temporal tuning curve approach naturally generalises the \NEF dynamics principle \citep[Chapter~8]{eliasmith2003neural}.
That is, using temporal tuning curves, we can realise any dynamical system that could be realised using the dynamics principle.
%\footnote{While it is possible to realise nonlinear dynamical systems using temporal tuning curves, we focus on linear dynamical systems for the sake of simplicity.}
At the same time, temporal tuning curves facilitate taking the temporal properties of pre-populations and intermediate filters into account, including heterogeneous and higher-order synapses.

However, the most important contribution of our temporal tuning curve approach is a shift in perspective.
Just as we harness non-temporal tuning curves and neural nonlinearities to approximate spatial functions in neural networks, we are now able to harness the temporal tuning of pre-populations and intermediate filters to realise the temporal tuning of post-populations.
Perhaps surprisingly, this includes recurrent neuron populations.
We furthermore separate \emph{temporal} from \emph{spatial} tuning.
A neuron can be tuned to a $d$-dimensional spatial quantity, and, at the same time, possess some temporal tuning, typically of finite order $q$.

Crucially, this allows modellers to directly specify some desired neural behaviour.
For example, a modeller could choose to assign time-cell like tuning \citep{pastalkova2008internally,tiganj2016sequential} to one population, and visual spatiotemporal receptive fields \citep{carandini1999linearity} to another.
In theory, these tuning properties can be provided in the form of sampled data.
So far, and as criticised by \citet{nicola2017supervised}, this was more difficult to accomplish using the \NEF dynamics principle---while the representation and transformation principle facilitate incorporating empirical data, the dynamics principle typically relies on modellers providing closed-form equations (although partially data-driven dynamics were possible, e.g., the hand-writing generation in SPAUN; cf.~\cite{eliasmith2012largescale}; \cite{choo2018spaun}, Section~3.2.2).

Almost more strikingly, temporal tuning is not just useful for modelling neurobiological systems.
Introducing the right kind of dynamics into \emph{artificial} neural networks results in architectures that outperform successful approaches in machine learning by wide margins.
Specifically, we show that a layer of Legendre Memory Units (\LMUpl; \cite{voelker2019lmu}) corresponds to a spatiotemporally tuned \NEF population.
%We use our insights from modelling biological systems to propose alternative variants of this system.

\subsubsection{Structure}
We start \Cref{sec:temporal_tuning_curves} with a review of \enquote{temporal tuning} in biology, and extend the \NEF by introducing the concept of a \enquote{temporal tuning curve}.
Specifically, we propose a linear least-squares optimisation problem that realises the desired temporal tuning by taking synaptic filters and the pre-population tuning into account.
Under ideal conditions, temporal tuning curves can be used to construct recurrent neural networks that realise \LTI systems while compensating for heterogeneous and higher-order synaptic filters.

Given that we can tune neurons to arbitrary \LTI systems, we ask in \Cref{sec:temporal_bases} what \LTI system we should optimally implement to obtain a maximally diverse set of temporal tuning curves.
This leads us to the concept of \LTI systems approximating sliding-window spectra.
We introduce a simple technique for constructing such \LTI systems and re-derive the Legendre Delay Network (\LDN) using this approach.
Furthermore, we discover a \enquote{modified Fourier system} as a potential alternative to the \LDN.

In \Cref{sec:recurrent_weights}, we demonstrate that our temporal tuning curve approach still works in less ideal spiking neural networks.
We show that, just like the \LDN, our modified Fourier system generates \enquote{time cells} when mapped onto a spiking neural network.
Furthermore, we use spatiotemporal encoding matrices to generate multi-dimensional spatiotemporal tuning and demonstrate decoding nonlinear functions over both space and time.

Finally, in \Cref{sec:applications_to_ml}, we draw parallels between \LMU layers and our aforementioned \NEF populations with spatiotemporal tuning.
We show that the choice of the sliding window-spectrum is secondary to the performance of the \LMU and demonstrate that convolving input signals with the modified Fourier system can offer a significant improvement in terms of asymptotic time-complexity compared to the \LDN.
We close with a discussion in \Cref{sec:temporal_tuning_conclusion}.

\begin{Notation}
We sometimes use Fraktur letters (e.g., $\mathfrak{a}$, $\mathfrak{b}$, \textellipsis) to convey that the mathematical objects we are referring to are now \emph{functions} over time.
That is, we write a temporal encoder as $\vec{\mathfrak{e}}$ instead of $\vec e$.
Note that $\vec{\mathfrak{e}}(t)$ refers to the result of evaluating the function $\vec{\mathfrak{e}}$ at a specific point in time $t$, whereas $\vec{\mathfrak{e}}$ refers to the function object as a whole.
In other words, $\vec{\mathfrak{e}}$ is a map $\mathbb{R} \longrightarrow \mathbb{R}^d$, whereas $\vec{\mathfrak{e}}(t)$ is an element in $\mathbb{R}^d$.
This distinction is important when expressing operations on functions, such as convolution.
We furthermore assume in most of our definitions that $t = 0$ corresponds to the current point in time.
%Since nervous systems typically do not have access to future events, the stimulus history $\vec{\mathfrak{x}}(t)$ only needs to be defined for $t \leq 0$.
\end{Notation}
