% !TeX spellcheck = en_GB

\section{Additional Data for Chapter~3}
\label{app:data_chp3}

This section contains additional tables and figures for the experiments conducted in Chapter~3.
\Cref{tbl:two_comp_neuron_parameters} lists neuron parameters used throughout \Cref{sec:two_comp_lif,sec:nlif_opt} unless explicitly stated differently.
The model parameters derived in \Cref{sec:two_comp_lif_experiment_1} are given in \Cref{tbl:two_comp_model_parameters}; functions used in the two-compartment \LIF network experiment are given in \Cref{tbl:two_comp_functions}.
\Cref{fig:nlif_parameters_contours_no_calibration} depicts rate approximation errors for more complex \nlif neurons without calibration.

\input{content/chapters/ZC_data/ZC_02_tbl_neuron_parameters}

\input{content/chapters/ZC_data/ZC_02_tbl_model_parameters}

\input{content/chapters/ZC_data/ZC_02_tbl_functions}

Extended results for our two-compartment \LIF network experiments are provided in \Cref{tbl:function_approximations_complete}.
All error values correspond to the \NRMSE (relative to the \RMS of the target function).
The error $E_\mathrm{model}$ corresponds to the static decoding errors assuming the dendritic model is accurate;
the error $E_\mathrm{net}$ is the decoding error after passing the input through the entire network.
The given errors are the mean and standard deviation over $N = 100$ experiments.
Also refer to \Cref{tbl:function_approximations} for a more detailed legend.

\input{content/chapters/ZC_data/ZC_02_tbl_two_comp_results}

\subsection{Regularisation Factor and Pre-Filter Sweep}
\label{app:two_comp_regularisation_factor_sweep}

The regularisation factors $\lambda$ used in the theoretical analysis of the two-compartment \LIF nonlinearity (\Cref{sec:two_comp_lif_experiment_2}) were determined by performing a parameter sweep with a spatial low-pass filter coefficient of $\rho^{-1} = 0.5$.
Results of this sweep are depicted in \Cref{fig:two_comp_2d_regularisation_sweep}.
\begin{figure}
	\centering
	\includegraphics{media/chapters/ZC_data/two_comp_2d_regularisation_sweep.pdf}
	%TODO Make sure the regularisation factors are called lambda everywhere; double-check the equations
	\caption[Regularisation sweep for the theoretical analysis of the two-compartment LIF nonlinearity]{Regularisation sweep for the theoretical analysis of the two-compartment \LIF nonlinearity (\Cref{sec:two_comp_lif_experiment_2}). Depicted lines are the median error over $N=128$ experiments for random 2D-functions with spatial filter $\rho^{-1} = 0.5$.
	%Black crosses correspond to the minimum error for each curve.
	\textbf{(A)} Is without subthreshold relaxation, \textbf{(B)} with subthreshold relaxation.}
	\label{fig:two_comp_2d_regularisation_sweep}
\end{figure}

Similarly, the regularisation factors used in the network experiment (\Cref{sec:two_comp_lif_experiment_3}) were determined independently for each network setup by sweeping over the regularisation factor $\lambda$, and choosing the $\lambda$ that minimises the network error $E_\mathrm{net}$ when computing multiplication.
We combine this sweep with the \enquote{pre-filter experiment} described below; results can be found in \Cref{fig:regularization_parameter_sweep_nosubth,fig:regularization_parameter_sweep_subth}.
The final parameters used in \Cref{sec:two_comp_lif_experiment_3} are listed in \Cref{tbl:regularization_paremteters}.

The purpose of the pre-filter experiment is the following.
As mentioned in the main text, the dendritic compartment in the two-compartment model can be interpreted as an additional first order low-pass filter with time-constant $C_\mathrm{m} g_\mathrm{L}^{-1}$ (cf.~\Cref{tbl:two_comp_neuron_parameters}).
This low-pass filter reduces the amplitude of high-frequency transients in the input signal and may thus contribute to the reduction of the final approximation error $E_\mathrm{net}$.

To explore the effects of low-pass filters in the network, we perform an additional experiment in which we introduced a first-order low-pass filter (the \enquote{pre-filter}) located at the input of the target population (for single-layer networks) or the input of the intermediate population (in the case of the two-layer network).
By optimising both the regularisation factor $\lambda$ and the filter time-constant $\tau$, each network setup has the chance to include an additional, optimal low-pass filter.
This rules out that the positive effects of the two-compartment neuron are primarily due to the low-pass filter behaviour of the dendritic compartment.
Notably, this low-pass filter is not included in the filter-chain for the target signal used to compute the estimation error.
Hence, the chosen filter time-constant must balance between suppressing noise in the input and not dampening high-frequency components in the target signal.

As depicted in \Cref{fig:regularization_parameter_sweep_nosubth,fig:regularization_parameter_sweep_subth}, the pre-filter does not result in any appreciable improvement in error for single layer networks with standard \LIF neurons.
For both the two-layer \LIF network, and the two-compartment \LIF neurons without noise model, the error is minimised for a filter coefficient of about \SIrange{10}{20}{\milli\second}.
As one would expect, when using the two-compartment \LIF noise model, adding a pre-filter is more detrimental than useful.

Function approximation errors are given in (cf.~\Cref{tbl:function_approximations_pre_filter}).
Again, we provide the mean and standard deviation of the model and network errors for $N = 100$ samples; see above for a complete description of the table.
The pre-filter reduces the error by only about $1$-$2\%$ across all network setups, which is far less than the observed reduction in error when using the two-compartment model compared to the standard \LIF neuron model.
We conclude that, while additional low-pass filters in the network can help to reduce the overall error, the reduction in error is too small to be an explanation for the smaller errors produced by the two-compartment model.

\begin{figure}[p]
	\includegraphics{media/chapters/ZC_data/two_comp_benchmark_functions_regularisation_filter_sweep_nosubth.pdf}
	\caption[Regularisation factor and pre-filter time-constant sweeps without subthreshold relaxation]{Regularisation factor and pre-filter time-constant sweeps without subthreshold relaxation. Contour plots are based on the median network error $E_\mathrm{net}$ over 32 samples on a $32 \times 33$ grid. Blue and orange crosses indicate the point with minimum error with and without pre-filter, respectively.}
	\label{fig:regularization_parameter_sweep_nosubth}
\end{figure}

\begin{figure}[p]
	\includegraphics{media/chapters/ZC_data/two_comp_benchmark_functions_regularisation_filter_sweep_subth.pdf}
	\caption[Regularisation factor and pre-filter time-constant sweep with subthreshold relaxation]{Regularisation factor and pre-filter time-constant sweep with subthreshold relaxation. Contour plots are based on the median network error $E_\mathrm{net}$ over 32 samples on a $32 \times 33$ grid. Blue and orange crosses indicate the point with minimum error with and without pre-filter, respectively.}
	\label{fig:regularization_parameter_sweep_subth}
\end{figure}

\begin{table}
	\caption[Regularisation factors and pre-filter time-constants]{Regularisation factors and pre-filter time-constants for the two-compartment \LIF network experiments. See text for a description.
	\textsuperscript{\dag}With subthreshold relaxation.
	}
	\label{tbl:regularization_paremteters}
	\centering
	\small
	\setlength{\tabcolsep}{10.75pt}
	\sffamily
	\begin{tabular}{r r l r l r r}
		\toprule
		\multicolumn{2}{c}{\textbf{Experiment setup}} &
		\multicolumn{2}{c}{\textbf{No pre-filter}} &
		\multicolumn{3}{c}{\textbf{With pre-filter}} \\
		\cmidrule{1-2}\cmidrule(l){3-4}\cmidrule(l){5-7}
		& & $\sigma$ & $E_\mathrm{net}$ & $\sigma$ & $\tau$ & $E_\mathrm{net}$ \\
		\midrule
		%--------------------
		\multirow{4}{2.7cm}[-0.5em]{\raggedleft LIF}
			& standard
			& $9.4 \times 10^{1}$ & $25.5\%$
			& $3.8 \times 10^{1}$ & $\SI{36.5}{\milli\second}$ & $25.0\%$ \\
			& standard\textsuperscript{\dag}
			& $6.8 \times 10^{1}$ & $15.4\%$
			& $3.8 \times 10^{1}$ & $\SI{20.6}{\milli\second}$ & $14.7\%$ \\
		\cmidrule{2-7}
			& two layers
			& $1.6 \times 10^{1}$ & $9.2\%$
			& $8.8 \times 10^{0}$ & $\SI{11.6}{\milli\second}$ & $8.4\%$ \\
			& two layers\textsuperscript{\dag}
			& $1.2 \times 10^{1}$ & $9.1\%$
			& $8.5 \times 10^{0}$ & $\SI{11.6}{\milli\second}$ & $8.4\%$ \\
		\midrule
		\multirow{4}{2.7cm}[-0.5em]{\raggedleft Two comp. LIF $c_{12} = \SI{50}{\nano\siemens}$}
			& standard
			& $1.2 \times 10^{-2}$ & $6.2\%$
			& $1.0 \times 10^{-3}$ & $\SI{13.5}{\milli\second}$ & $4.9\%$ \\
			& standard\textsuperscript{\dag}
			& $1.1 \times 10^{-1}$ & $4.3\%$
			& $1.5 \times 10^{-3}$ & $\SI{11.6}{\milli\second}$ & $3.5\%$ \\
		\cmidrule{2-7}
			& noise model
			& $1.2 \times 10^{-3}$ & $5.4\%$
			& $1.2 \times 10^{-3}$ & $\SI{1.0}{\milli\second}$ & $5.4\%$ \\
			& noise model\textsuperscript{\dag}
			& $1.0 \times 10^{-3}$ & $4.7\%$
			& $1.0 \times 10^{-3}$ & $\SI{1.0}{\milli\second}$ & $4.7\%$ \\
		\midrule
		\multirow{4}{2.7cm}[-0.5em]{\raggedleft Two comp. LIF $c_{12} = \SI{100}{\nano\siemens}$}
			& standard
			& $5.5 \times 10^{-2}$ & $7.0\%$
			& $1.0 \times 10^{-3}$ & $\SI{15.3}{\milli\second}$ & $4.8\%$ \\
			& standard\textsuperscript{\dag}
			& $1.7 \times 10^{-1}$ & $5.2\%$
			& $4.8 \times 10^{-3}$ & $\SI{13.3}{\milli\second}$ & $4.0\%$ \\
		\cmidrule{2-7}
			& noise model
			& $1.0 \times 10^{-3}$ & $8.0\%$
			& $1.0 \times 10^{-3}$ & $\SI{1.0}{\milli\second}$ & $8.0\%$ \\
			& noise model\textsuperscript{\dag}
			& $8.5 \times 10^{0}$ & $6.8\%$
			& $8.5 \times 10^{0}$ & $\SI{1.8}{\milli\second}$ & $6.8\%$ \\
		\midrule
		\multirow{4}{2.7cm}[-0.5em]{\raggedleft Two comp. LIF $c_{12} = \SI{200}{\nano\siemens}$}
			& standard
			& $3.6 \times 10^{-2}$ & $8.9\%$
			& $1.0 \times 10^{-3}$ & $\SI{23.7}{\milli\second}$ & $7.0\%$ \\
			& standard\textsuperscript{\dag}
			& $4.4 \times 10^{-2}$ & $7.0\%$
			& $3.2 \times 10^{-1}$ & $\SI{17.9}{\milli\second}$ & $5.7\%$ \\
		\cmidrule{2-7}
			& noise model
			& $2.0 \times 10^{0}$ & $14.3\%$
			& $2.0 \times 10^{0}$ & $\SI{1.0}{\milli\second}$ & $14.3\%$ \\
			& noise model\textsuperscript{\dag}
			& $8.5 \times 10^{0}$ & $9.1\%$
			& $8.8 \times 10^{0}$ & $\SI{17.6}{\milli\second}$ & $9.0\%$ \\
		%------------------
		\bottomrule
	\end{tabular}
\end{table}

\input{content/chapters/ZC_data/ZC_02_tbl_two_comp_pre_filter_results}

\begin{figure}
	\includegraphics{media/chapters/ZC_data/nlif_parameters_contours_no_calibration.pdf}
	\caption[Uncalibrated rate predictions for different $n$-LIF neurons]{Uncalibrated rate predictions for different $n$-LIF neurons. Same data as in \Cref{fig:nlif_parameters_contours}, but without calibration of the neuron. Calibration reduces the prediction errors by a factor of two to four.}
	\label{fig:nlif_parameters_contours_no_calibration}
\end{figure}

\begin{sidewaystable}
	\centering
	\caption[Calibrated and conditioned reduced $n$-LIF system matrices]{Calibrated and conditioned reduced $n$-LIF system matrices for the experiments in \Cref{sec:nlif_opt}. Letters correspond to the neurons depicted in \Cref{fig:nlif}: \emph{(B)} one-compartment with conductance channels, \emph{(C)} two, \emph{(D)} three, and \emph{(E)} four compartments.}

	\label{tbl:nlif_params}

	\small\sffamily
	\renewcommand{\arraystretch}{1.3}
	\scalebox{0.89}{\begin{tabular}{c c c c c c c}

		\toprule

		\multicolumn{1}{c}{\textbf{Model}\!\!} & $\vec{\tilde a}'$ & $\mat{\tilde A}'$ & $\vec{\tilde b}'$ & $\mat{\tilde B}'$ & $\mat{\tilde L}$ & $\vec{\tilde c}$\\

		\midrule

		\multicolumn{7}{c}{\footnotesize\textbf{Before calibration}}\\
		\midrule
			\textbf{(B)}

			& $\displaystyle \begin{bmatrix} 1 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 & 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 57.5 & -17.5 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \end{bmatrix}$\\[1em]
			\textbf{(C)}

			& $\displaystyle \begin{bmatrix}

				1 \\ 2
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 \\

				20 & 20 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 \\

				-0.375
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 \\

				57.5 & -17.5
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \\ 1 \end{bmatrix}$\\[1em]
			\textbf{(D)} 

			& $\displaystyle \begin{bmatrix}

				1 \\

				1.5\\

				1
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 \\

				10 & 10 & 0 & 0 \\

				0 & 0 & 20 & 20 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 \\ -0.375 \\ -0.375
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 \\

				57.5 & -17.5 & 0 & 0 \\

				0 & 0 & 57.5 & -17.5 \\

			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 \\

				0 & 2 & -4 \\

				0 & -2 & 4 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$\\[1em]
			\textbf{(E)} 

			& $\displaystyle \begin{bmatrix}

				1 \\

				1.5\\

				1\\
				1
			\end{bmatrix}$

%			& $\displaystyle \begin{bmatrix}

%				0 & 10 & 0 & 0 \\
%				0 & 10 & 0 & 0\\
%				0 & 0 & 20 & 0\\
%				0 & 0 & 20 & 0\\
%				0 & 0 & 0 & 20 \\
%				0 & 0 & 0 & 20
%			\end{bmatrix}^T$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 & 0 & 0 \\
				10 & 10 & 0 & 0 & 0 & 0 \\
				0 & 0 & 20 & 20 & 0 & 0 \\
				0 & 0 & 0 & 0 & 20 & 20
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				-0.375 \\ -0.375 \\ -0.375 \\ -0.375
			\end{bmatrix}$

%			& $\displaystyle \begin{bmatrix}
%				0 & 57.5 & 0 & 0\\
%				0 & -17.5 & 0 & 0\\
%				0 & 0 & 57.5 & 0\\
%				0 & 0 & -17.5 & 0\\
%				0 & 0 & 0 & 57.5 \\
%				0 & 0 & 0 & -17.5
%			\end{bmatrix}^T$

			& $\displaystyle \begin{bmatrix}
				0 & 0 & 0 & 0 & 0 & 0 \\
				57.5 & -17.5 & 0 & 0 & 0 & 0 \\
				0 & 0 & 57.5 & -17.5 & 0 & 0 \\
				0 & 0 & 0 & 0 & 57.5 & -17.5 \\
			\end{bmatrix}$
			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 \\

				0 & 2 & -4 & 0 \\

				0 & -2 & 14 & -10 \\
				0 & 0 & -10 & 10 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}$
			\\
			\midrule
			\multicolumn{7}{c}{\footnotesize\textbf{After calibration}}\\
			\midrule
			\textbf{(B)}

			& $\displaystyle \begin{bmatrix} 1 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 & 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} -0.82 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 64.65 & -23.08 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \end{bmatrix}$\\[1em]
			\textbf{(C)}

			& $\displaystyle \begin{bmatrix}

				1 \\ 0.59
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 \\

				10.50 & 2.90 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				-2.67 \\

				0.79
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 \\

				55.97 & -11.67
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \\ 1 \end{bmatrix}$\\[1em]
			\textbf{(D)} 

			& $\displaystyle \begin{bmatrix}

				1 \\

				0.82\\

				1.40
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 \\

				9.75 & 4.94 & 0 & 0 \\

				0 & 0 & 19.07 & 16.87 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				-0.53 \\ -0.91 \\ 0.10
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 \\

				57.63 & -24.93 & 0 & 0 \\

				0 & 0 & 55.42 & -16.06 \\

			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 \\

				0 & 2 & -4 \\

				0 & -2 & 4 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}$\\[1em]
			\textbf{(E)} 

			& $\displaystyle \begin{bmatrix}

				1 \\

				0.68\\

				2.69\\
				0.54
			\end{bmatrix}$

%			& $\displaystyle \begin{bmatrix}

%				0 & 10.20 & 0 & 0 \\
%				0 & 4.66 & 0 & 0\\
%				0 & 0 & 18.35 & 0\\
%				0 & 0 & 15.12 & 0\\
%				0 & 0 & 0 & 15.11 \\
%				0 & 0 & 0 & 10.83
%			\end{bmatrix}^T$

			& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 & 0 & 0 \\
				10.20 & 4.66 & 0 & 0 & 0 & 0 \\
				0 & 0 & 18.35 & 15.12 & 0 & 0 \\
				0 & 0 & 0 & 0 & 15.11 & 10.83
			\end{bmatrix}$

			& $\displaystyle \begin{bmatrix}

				-0.45 \\ -1.95 \\ -0.20 \\ 2.40
			\end{bmatrix}$

%			& $\displaystyle \begin{bmatrix}
%				0 & 58.34 & 0 & 0\\
%				0 & -26.18 & 0 & 0\\
%				0 & 0 & 56.47 & 0\\
%				0 & 0 & -15.81 & 0\\
%				0 & 0 & 0 & 51.06 \\
%				0 & 0 & 0 & -11.89
%			\end{bmatrix}^T$

			& $\displaystyle \begin{bmatrix}
				0 & 0 & 0 & 0 & 0 & 0 \\
				58.34 & -26.18 & 0 & 0 & 0 & 0 \\
				0 & 0 & 56.47 & -15.81 & 0 & 0 \\
				0 & 0 & 0 & 0 & 51.06 & -11.89
			\end{bmatrix}$

		& $\displaystyle \begin{bmatrix}

				0 & 0 & 0 & 0 \\

				0 & 2 & -4 & 0 \\

				0 & -2 & 14 & -10 \\
				0 & 0 & -10 & 10 \end{bmatrix}$

			& $\displaystyle \begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \end{bmatrix}$\\
		\bottomrule

	\end{tabular}}
\end{sidewaystable}


\begin{table}
\centering\vspace{0.5cm}
\caption[Detailed function approximation errors for spiking networks using various $n$-LIF neurons]{Detailed function approximation errors for spiking networks using various $n$-LIF neurons.
Top half of the table contains the static function approximation errors $E_\mathrm{model}$.
These are the network errors obtained assuming that there is no spike noise and that our rate approximations are correct.
See \Cref{tbl:function_approximations_nlif} for more details.
Bottom half contains the minimum network errors $E_\mathrm{net}$ over $100$ trials.
}
\label{tbl:function_approximations_nlif_model}
\fontsize{10pt}{12pt}\selectfont
\setlength{\tabcolsep}{10pt}
\renewcommand\arraystretch{1.12}
\sffamily
\begin{tabular}{r r r r r r r }
\toprule
\textbf{Function} & \textbf{Domain} & \multicolumn{5}{c}{\textbf{Neuron}} \\
\cmidrule(r){1-1}\cmidrule(r){2-2}\cmidrule{3-7}
&
&	 \multicolumn{2}{c}{LIF}
&	 \multicolumn{3}{c}{$n$-LIF}
\\
\cmidrule(r){3-4}\cmidrule{5-7}
&
&	 \multicolumn{1}{c}{standard}
&	 \multicolumn{1}{c}{two layers}
&	 \multicolumn{1}{c}{$n = 2$}
&	 \multicolumn{1}{c}{$n = 3$}
&	 \multicolumn{1}{c}{$n = 4$}
\\
%--------------------------------------------

\midrule
\multicolumn{7}{c}{\textbf{Static function approximation errors}}\\
%\midrule
\multicolumn{7}{c}{\textit{Standard parameters} ($\lambda = 10^{-3}$; $\xi_0 \in [-0.95, 0.95]$; with Dale's principle, $p_\mathrm{inh} = 30\%$)} \\

\midrule

$x_1 + x_2$

& $[-1, 1]^2$

& \cellcolor{White!100!SteelBlue}{$\mathbf{2.1 \pm 0.6 \%}$}

& \cellcolor{White!80!SteelBlue}{$2.2 \pm 0.4 \%$}

& \cellcolor{White!20!SteelBlue}{$2.7 \pm 0.6 \%$}

& \cellcolor{White!80!SteelBlue}{$2.2 \pm 0.6 \%$}

& \cellcolor{White!80!SteelBlue}{$2.2 \pm 0.6 \%$}

\\

$x_1 / (1 + x_2)$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$8.9 \pm 0.8 \%$}

& \cellcolor{White!60!SteelBlue}{$3.3 \pm 0.4 \%$}

& \cellcolor{White!40!SteelBlue}{$3.8 \pm 1.0 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.1 \pm 1.0 \%}$}

& \cellcolor{White!80!SteelBlue}{$3.2 \pm 1.0 \%$}

\\

$\sqrt{x_1 \times x_2}$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$12.4 \pm 1.1 \%$}

& \cellcolor{White!40!SteelBlue}{$7.5 \pm 0.5 \%$}

& \cellcolor{White!60!SteelBlue}{$6.1 \pm 0.8 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{5.1 \pm 0.8 \%}$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{5.1 \pm 0.8 \%}$}

\\

$x_1 \times x_2$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$14.6 \pm 0.9 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{2.3 \pm 0.3 \%}$}

& \cellcolor{White!40!SteelBlue}{$2.8 \pm 0.5 \%$}

& \cellcolor{White!80!SteelBlue}{$2.4 \pm 0.4 \%$}

& \cellcolor{White!80!SteelBlue}{$2.4 \pm 0.4 \%$}

\\

$x_1 \times x_2$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$107.1 \pm 1.8 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{5.7 \pm 0.5 \%}$}

& \cellcolor{White!40!SteelBlue}{$77.2 \pm 3.6 \%$}

& \cellcolor{White!60!SteelBlue}{$39.0 \pm 4.6 \%$}

& \cellcolor{White!80!SteelBlue}{$38.2 \pm 5.0 \%$}

\\

$(x_1 \times x_2) ^ 2$

& $[-1, 1]^2$

& \cellcolor{White!40!SteelBlue}{$15.3 \pm 1.6 \%$}

& \cellcolor{White!80!SteelBlue}{$8.3 \pm 1.0 \%$}

& \cellcolor{White!20!SteelBlue}{$16.4 \pm 1.8 \%$}

& \cellcolor{White!60!SteelBlue}{$8.7 \pm 1.9 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{8.2 \pm 1.8 \%}$}

\\

$\|(x_1, x_2)\|$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$9.0 \pm 0.6 \%$}

& \cellcolor{White!60!SteelBlue}{$5.7 \pm 0.8 \%$}

& \cellcolor{White!40!SteelBlue}{$6.6 \pm 0.5 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{4.0 \pm 0.6 \%}$}

& \cellcolor{White!80!SteelBlue}{$4.2 \pm 0.6 \%$}

\\

$\mathrm{atan}(x_1, x_2)$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$35.2 \pm 2.4 \%$}

& \cellcolor{White!40!SteelBlue}{$29.2 \pm 3.3 \%$}

& \cellcolor{White!60!SteelBlue}{$15.3 \pm 4.3 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{13.1 \pm 4.9 \%}$}

& \cellcolor{White!80!SteelBlue}{$13.3 \pm 4.4 \%$}

\\

$\max(x_1, x_2)$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$22.3 \pm 1.1 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.4 \pm 0.3 \%}$}

& \cellcolor{White!40!SteelBlue}{$8.0 \pm 0.5 \%$}

& \cellcolor{White!80!SteelBlue}{$5.4 \pm 0.5 \%$}

& \cellcolor{White!60!SteelBlue}{$5.7 \pm 0.6 \%$}

\\

\midrule

\multicolumn{7}{c}{\textit{Adapted parameters} ($\lambda = 10^{-6}$; $\xi_0 \in [-0.95, 0]$; no Dale's principle)} \\

\midrule

$x_1 + x_2$

& $[-1, 1]^2$

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.4 \pm 0.4 \%}$}

& \cellcolor{White!40!SteelBlue}{$3.5 \pm 0.3 \%$}

& \cellcolor{White!20!SteelBlue}{$3.9 \pm 0.4 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.4 \pm 0.4 \%}$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.4 \pm 0.4 \%}$}

\\

$x_1 \times x_2$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$20.4 \pm 0.9 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{4.7 \pm 0.7 \%}$}

& \cellcolor{White!40!SteelBlue}{$5.4 \pm 0.6 \%$}

& \cellcolor{White!80!SteelBlue}{$5.0 \pm 0.7 \%$}

& \cellcolor{White!80!SteelBlue}{$5.0 \pm 0.7 \%$}

\\

$x_1 \times x_2$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$109.7 \pm 2.6 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{6.5 \pm 0.3 \%}$}

& \cellcolor{White!40!SteelBlue}{$71.6 \pm 4.9 \%$}

& \cellcolor{White!60!SteelBlue}{$26.1 \pm 5.0 \%$}

& \cellcolor{White!80!SteelBlue}{$24.1 \pm 4.0 \%$}

\\

\midrule
\multicolumn{7}{c}{\textbf{Minimum network errors}}\\
%\midrule
\multicolumn{7}{c}{\textit{Standard parameters} ($\lambda = 10^{-3}$; $\xi_0 \in [-0.95, 0.95]$; with Dale's principle, $p_\mathrm{inh} = 30\%$)} \\

\midrule

$x_1 + x_2$

& $[-1, 1]^2$

& \cellcolor{White!80!SteelBlue}{$3.8 \%$}

& \cellcolor{White!20!SteelBlue}{$8.4 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.3 \%}$}

& \cellcolor{White!60!SteelBlue}{$4.4 \%$}

& \cellcolor{White!40!SteelBlue}{$5.9 \%$}

\\

$x_1 / (1 + x_2)$

& $[0, 1]^2$

& \cellcolor{White!40!SteelBlue}{$8.0 \%$}

& \cellcolor{White!20!SteelBlue}{$8.4 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.9 \%}$}

& \cellcolor{White!80!SteelBlue}{$5.6 \%$}

& \cellcolor{White!60!SteelBlue}{$6.3 \%$}

\\

$\sqrt{x_1 \times x_2}$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$12.6 \%$}

& \cellcolor{White!40!SteelBlue}{$10.2 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{4.5 \%}$}

& \cellcolor{White!80!SteelBlue}{$5.3 \%$}

& \cellcolor{White!60!SteelBlue}{$6.7 \%$}

\\

$x_1 \times x_2$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$15.1 \%$}

& \cellcolor{White!40!SteelBlue}{$7.5 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{3.7 \%}$}

& \cellcolor{White!80!SteelBlue}{$3.9 \%$}

& \cellcolor{White!60!SteelBlue}{$5.9 \%$}

\\

$x_1 \times x_2$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$100.0 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{11.5 \%}$}

& \cellcolor{White!40!SteelBlue}{$75.1 \%$}

& \cellcolor{White!60!SteelBlue}{$31.3 \%$}

& \cellcolor{White!80!SteelBlue}{$29.2 \%$}

\\

$(x_1 \times x_2) ^ 2$

& $[-1, 1]^2$

& \cellcolor{White!60!SteelBlue}{$15.8 \%$}

& \cellcolor{White!40!SteelBlue}{$16.8 \%$}

& \cellcolor{White!20!SteelBlue}{$17.0 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{10.2 \%}$}

& \cellcolor{White!80!SteelBlue}{$10.4 \%$}

\\

$\|(x_1, x_2)\|$

& $[-1, 1]^2$

& \cellcolor{White!40!SteelBlue}{$10.2 \%$}

& \cellcolor{White!20!SteelBlue}{$12.6 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{5.9 \%}$}

& \cellcolor{White!80!SteelBlue}{$6.2 \%$}

& \cellcolor{White!60!SteelBlue}{$8.6 \%$}

\\

$\mathrm{atan}(x_1, x_2)$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$37.0 \%$}

& \cellcolor{White!40!SteelBlue}{$33.7 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{15.0 \%}$}

& \cellcolor{White!80!SteelBlue}{$17.0 \%$}

& \cellcolor{White!60!SteelBlue}{$18.0 \%$}

\\

$\max(x_1, x_2)$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$21.8 \%$}

& \cellcolor{White!40!SteelBlue}{$8.1 \%$}

& \cellcolor{White!80!SteelBlue}{$6.7 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{6.4 \%}$}

& \cellcolor{White!60!SteelBlue}{$7.3 \%$}

\\

\midrule

\multicolumn{7}{c}{\textit{Adapted parameters} ($\lambda = 10^{-6}$; $\xi_0 \in [-0.95, 0]$; no Dale's principle)} \\

\midrule

$x_1 + x_2$

& $[-1, 1]^2$

& \cellcolor{White!100!SteelBlue}{$\mathbf{4.0 \%}$}

& \cellcolor{White!60!SteelBlue}{$7.1 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{4.0 \%}$}

& \cellcolor{White!40!SteelBlue}{$20.3 \%$}

& \cellcolor{White!20!SteelBlue}{$22.8 \%$}

\\

$x_1 \times x_2$

& $[0, 1]^2$

& \cellcolor{White!20!SteelBlue}{$21.3 \%$}

& \cellcolor{White!80!SteelBlue}{$9.0 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{4.6 \%}$}

& \cellcolor{White!60!SteelBlue}{$9.3 \%$}

& \cellcolor{White!40!SteelBlue}{$9.8 \%$}

\\

$x_1 \times x_2$

& $[-1, 1]^2$

& \cellcolor{White!20!SteelBlue}{$99.1 \%$}

& \cellcolor{White!100!SteelBlue}{$\mathbf{10.3 \%}$}

& \cellcolor{White!40!SteelBlue}{$65.0 \%$}

& \cellcolor{White!80!SteelBlue}{$14.7 \%$}

& \cellcolor{White!60!SteelBlue}{$14.9 \%$}

\\

\bottomrule

\end{tabular}

%--------------------------------------------

\end{table}
