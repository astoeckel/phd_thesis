% !TeX spellcheck = en_GB

\section{Additional Data for Chapter~4}
\label{app:data_chp4}

\subsection{Comparison between the LDN and Modified Fourier Systems}
\label{sec:ldn_mfn_basis_bandlimit}

\Citet[Section~6.1.1]{voelker2019} notes that the \LDN system optimally approximates a time-delay for low-frequency inputs generated by an \LTI system of order $p + q = 2q - 1$.
However, for our experiments in \Cref{sec:comparing_temporal_bases}, we used low-pass filtered white noise, which technically consists of infinitely many (albeit severely dampened) frequency components.
We found that \LTI systems generating a modified Fourier basis can outperform the \LDN in this scenario.

As is depicted in \Cref{fig:delay_analysis_example_bandlimited,fig:delay_analysis_bandlimited}, we test whether switching to a strictly band-limited inputs $u(t)$ impacts the performance of these two systems.
Indeed, using strictly band-limited signals substantially reduces the errors for both the modified Fourier system and the \LDN.
However, the modified Fourier system still significantly outperforms the \LDN.

\vspace{0.125cm}

\begin{figure}[h]
	\centering
	\includegraphics{media/chapters/ZC_data/delay_analysis_example_bandlimited.pdf}
	\caption[Example delay decodings for strictly bandlimited input]{Example delay decodings for strictly band-limited input.
	Same figure as \Cref{fig:delay_analysis_example}, but with an input $u(t)$ that is strictly band-limited to \SI{5}{\hertz}.
	The number of state dimensions is $q = 11$.
	}
	\label{fig:delay_analysis_example_bandlimited}
\end{figure}

\begin{figure}[h]
	\centering
	\includegraphics{media/chapters/ZC_data/delay_analysis_bandlimited.pdf}
	\caption[Comparing the modified Fourier and the LDN systems for band-limited inputs]{Comparing the modified Fourier and the \LDN systems for band-limited inputs $u(t)$. The analysis is as in \Cref{fig:delay_analysis_overview,fig:delay_analysis_boxplots}, but for a strictly band-limited white noise signal with a maximum frequency of \SI{10}{\hertz} over $T = \SI{10}{\second}$.
	Note that adding more basis functions beyond $q = 20$ barely improves the accuracy of the Fourier-based system.
	Two stars indicate $p < 0.01$, three stars $p < 0.001$.}
	\label{fig:delay_analysis_bandlimited}
\end{figure}


\subsection{Realising Spatiotemporal Networks Without Temporal Tuning Curves}
\label{app:spatiotemporal_nef}

\begin{figure}[b]
	\centering
	\includegraphics{media/chapters/04_temporal_tuning/spatio_temporal_analysis_nef.pdf}
	\caption[Results of a delay decoding experiment in a spatiotemporal NEF network]{Results of a delay decoding experiment in a spatiotemporal \NEF network. Same plots as in \Cref{fig:spatio_temporal_full}.
	\textbf{(A, B)} Both the mean delay decoding and mean delayed multiplication errors are slightly larger compared to our temporal tuning curve implementation.
	\textbf{(C)} The singular values of the matrix $\mat A'$ are close those of the recurrent weight matrix $\mat W_\mathrm{rec}$.
	}
	\label{fig:spatio_temporal_analysis_nef}
\end{figure}

We mentioned in \Cref{sec:spatiotemporal} that it is possible to implement spatiotemporal neuron populations in the \NEF without explicitly relying on temporal tuning curves.
Essentially, we merely need to realise the same \LTI system $d$ times in the population---flattening the encoder matrices $\mat E^\mathrm{t}_i$ then results in the corresponding \NEF encoding vectors $\vec e_i$.

Let $d$ be the number of spatial dimensions, $q$ the number of temporal dimensions, and $\mat A$, $\mat B$ be the state-space matrices generating the desired temporal basis.
Applying the \NEF dynamics principle, we need to realise the following linear input and feedback transformations:
\begin{align*}
	\mat B' &= \tau \begin{pmatrix}
		\mat B & \hdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \hdots & \mat B
	\end{pmatrix} \,, &
	\mat A' &= \tau \begin{pmatrix}
		\mat A & \hdots & 0 \\
		\vdots & \ddots & \vdots \\
		0 & \hdots & \mat A
	\end{pmatrix} + \mat I \,,
\end{align*}
where $\tau$ is the time-constant of the first-order synaptic filter, $\mat B' \in \mathbb{R}^{dq \times d}$, and $\mat A' \in \mathbb{R}^{dq \times dq}$.

We repeat our delayed multiplication experiment from \Cref{sec:spatiotemporal} using this technique (see \Cref{fig:spatio_temporal_analysis_nef}).
We find that errors are slightly higher compared to using our temporal tuning curve approach.
This is mostly due to using unbiased samples $\vec x_k$ when solving for connection weights in a (relatively) high-dimensional space.
We typically use representative input signals when minimising the temporal tuning curve loss \cref{eqn:weight_optimise_currents_temporal}.
This biases the weight solver towards actually observed portions of the activity space (cf.~\Cref{sec:solve_dynamics_nonlinear_neurons}).
Attempting to sample $\vec x_k$ from this biased distribution amounts to the same computation as solving \cref{eqn:weight_optimise_currents_temporal} directly.

\subsection{Code and Data for the Machine Learning Experiments}
\label{app:lmu_code}

Below, we provide the source code describing the neural network architectures used in \Cref{sec:lmu_experiments}.
The code relies on TensorFlow 2.5 \citep{abadi2016tensorflow} and uses its Keras \API \citep{chollet2017deep} to describe the network.
The \texttt{TemporalBasisTrafo} layer is our own work.%
\footnote{See \url{https://github.com/astoeckel/temporal_basis_transformation_network} for more information.}

\subsubsection{psMNIST}

We use the following code to describe the network performing the psMNIST task:
\begin{pythoncode}
q = 468; N = 28 * 28; n = 346; H = mk_basis(q, N)
model = tf.keras.models.Sequential([
  tf.keras.layers.Reshape((N, 1)),             # (N, 1)
  TemporalBasisTrafo(H, units=1),              # (1, q)
  tf.keras.layers.Dropout(0.5),                # (1, q)
  tf.keras.layers.Dense(n, activation="relu"), # (1, M)
  tf.keras.layers.Dense(10, use_bias=False),   # (1, 10)
  tf.keras.layers.Reshape((10,))               # (10)
])
\end{pythoncode}
Comments \texttt{(Nt, Nd)} indicate the output dimensions of each layer.
Here, \texttt{Nt} is the number of temporal samples; \texttt{Nd} is the number of spatial dimensions.
The \texttt{TemporalBasisTrafo} layer consumes $N - 1$ temporal dimensions and converts them into $q$ spatial dimensions, where $N$ and $q$ are determined from the filter matrix $\mat H \in \mathbb{R}^{q \times N}$.
The function \texttt{mk\_basis} generates a set of $q$ \FIR filters of length $N$ corresponding to one of our basis transformation methods.%
\footnote{We provide a handy library that generates different basis transformation matrices at \url{https://github.com/astoeckel/dlop_ldn_function_bases}. See \citet{stockel2021discrete} for more information.}

\subsubsection{Mackey-Glass}

\begin{figure}
	\includegraphics{media/chapters/ZC_data/lmu_mackey_glass_filters.pdf}
	\caption[FIR filters used in the Mackey-Glass experiment]{\FIR filters used in the Mackey-Glass experiment. Each filter is three times as long as $q$. The impulse response of the \LDN system and the modified Fourier basis extends beyond $q$.}
	\label{fig:lmu_mackey_glass_filters}
\end{figure}

The network used for solving the Mackey-Glass task consists of four \LMU layers.
We extend the filter matrix $\mat H$ to be $\texttt{Wm} = 3$ times as long as the window width $N$ (cf.~\Cref{fig:lmu_mackey_glass_filters}).
This is reflected in the call to the \texttt{mk\_ext\_basis} function and ensures that the ringing artefacts from the \LDN and the modified Fourier basis are taken into account.
The \FIR filters for the \enquote{perfect} sliding-window spectra are padded with zeros; this merely introduces superfluous multiplications, but has no further effect on the computation.
\begin{pythoncode}
Wm = 3 # Window multiplier; the \FIR filter is this times as long as the window
N_units0, N_units1, N_units2, N_units3 = 1, 10, 10, 10
N_wnd0, N_wnd1, N_wnd2, N_wnd3 = N_wnds = (17 * Wm, 9 * Wm, 9 * Wm, 5 * Wm)
N_wnd = N_wnd0 + N_wnd1 + N_wnd2 + N_wnd3 - 3
q0, q1, q2, q3 = 17, 9, 9, 5
H0, H1 = mk_ext_basis(q0, N_wnd0 // Wm, Wm), mk_ext_basis(q1, N_wnd1 // Wm, Wm)
H2, H3 = mk_ext_basis(q2, N_wnd2 // Wm, Wm), mk_ext_basis(q3, N_wnd3 // Wm, Wm)
model = tf.keras.models.Sequential([
  tf.keras.layers.Reshape((N_wnd, 1)),
  # (N_wnd0 + N_wnd1 + N_wnd2 + N_wnd3 - 3, 1)
  TemporalBasisTrafo(H0, n_units=N_units0),
  # (N_wnd1 + N_wnd2 + N_wnd3 - 2, q0 * N_units0)
  tf.keras.layers.Dense(N_units1, activation="relu"),
  # (N_wnd1 + N_wnd2 + N_wnd3 - 2, N_units1)
  TemporalBasisTrafo(H1, n_units=N_units1),
  # (N_wnd2 + N_wnd3 - 1, q1 * N_units1)
  tf.keras.layers.Dense(N_units2, activation="relu"),
  # (N_wnd2 + N_wnd3 - 1, N_units2)
  TemporalBasisTrafo(H2, n_units=N_units2),
  # (N_wnd3, q2 * N_units2)
  tf.keras.layers.Dense(N_units3, activation="relu"),
  # (N_wnd3, N_units3)
  TemporalBasisTrafo(H3, n_units=N_units3),
  # (1, q3 * N_units3)
  tf.keras.layers.Dense(N_pred, use_bias=False), # (1, N_pred)
  tf.keras.layers.Reshape((N_pred,)) # (N_pred)
])
\end{pythoncode}
Comments \texttt{(Nt, Nd)} indicate the output dimensions of each layer, where \texttt{Nt} is the number of temporal, and \texttt{Nd} the number of spatial dimensions.
Each \texttt{TemporalBasisTrafo} consumes $\texttt{N\_wndX} - 1$ temporal dimensions, and replaces them by $\texttt{qX} * \texttt{N\_unitsX}$ spatial dimensions.

\begin{figure}
	\sffamily\small
	\centering
	\textbf{psMNIST experiment}\\[0.125cm]
	\includegraphics{media/chapters/ZC_data/lmu_psmnist_trajs.pdf}\\[0.25cm]
	\textbf{Mackey-Glass experiment}\\[0.125cm]
	\includegraphics{media/chapters/ZC_data/lmu_mackey_glass_trajs.pdf}
	\caption[Learning curves for the psMNIST and Mackey-Glass experiments]{Learning curves for the psMNIST and Mackey-Glass experiments. $y$-axis is the value of the loss function (categorical cross-entropy and logarithmic \MSE, respectively; scale is the same across all plots). Lines are the median over $101$ trials; shaded areas are the 10th and 90th percentiles. Blue circle indicates the minimum median validation error.}
	\label{fig:lmu_trajs}
\end{figure}


\begin{figure}[p]
\centering
\includegraphics{media/chapters/ZC_data/mackey_glass_results_not_extended.pdf}
\caption[Prediction errors for the Mackey-Glass dataset with perfect rectangle windows]{Prediction errors for the Mackey-Glass dataset with perfect rectangle windows. See \Cref{fig:psmnist_results} for a description of the plot.
Numerical values are given in \Cref{tbl:mackey_glass_results_ne}.}
\label{fig:mackey_glass_ne}
\end{figure}

\begin{table}[p]
	\newcommand{\sigA}{\ensuremath{\cdot}}
	\newcommand{\sigB}{\ensuremath{\bullet\bullet}}
	\newcommand{\sigC}{\ensuremath{\bullet\!\bullet\!\bullet}}
	\caption[Prediction errors and statistical analysis for the Mackey-Glass dataset]{Prediction errors and statistical analysis for the Mackey-Glass dataset with perfect rectangle window. Data over $101$ trials after $100$ epochs of training. See \Cref{tbl:psmnist_results,tbl:psmnist_significance} for a more detailed description.
	The given significance levels are based on a two-sided Kolmogorov-Smirnov test; $\sigA \correspondsTo p < 0.05$, $\sigB \correspondsTo p < 0.01$, $\sigC \correspondsTo p < 0.001$.
	}
	\label{tbl:mackey_glass_results_ne}
	\centering\small\sffamily
	\setlength{\tabcolsep}{9.68pt}
	\begin{tabular}{r  r r r r  r r r r}
	\toprule
	& \multicolumn{4}{c}{{\color{skyblue1}$\blacksquare$} \textbf{Fixed convolution}}
	& \multicolumn{4}{c}{{\color{aluminium2}$\blacksquare$} \textbf{Learned convolution}} \\
	\cmidrule(r){2-5}\cmidrule(l){6-9}
	\symLTI~LDN &
	 \cellcolor{CornflowerBlue!75}{4.02\%} &
	 \cellcolor{CornflowerBlue!25}{3.88\%} &
	 \cellcolor{CornflowerBlue!25}{3.35\%} &
	 \cellcolor{CornflowerBlue!25}{4.58\%} &
	 \cellcolor{CornflowerBlue!75}{3.80\%} &
	 \cellcolor{CornflowerBlue!50}{3.64\%} &
	 \cellcolor{CornflowerBlue!75}{3.09\%} &
	 \cellcolor{CornflowerBlue!25}{4.37\%} \\
	\symLTI~Mod.~Fourier &
	4.56\% &
	4.46\% &
	3.67\% &
	5.17\% &
	3.89\% &
	3.70\% &
	 \cellcolor{CornflowerBlue!50}{3.15\%} &
	4.39\% \\
	\symSDT~Fourier &
	4.22\% &
	4.11\% &
	3.50\% &
	4.78\% &
	3.93\% &
	3.83\% &
	3.32\% &
	4.52\% \\
	\symSDT~Cosine &
	 \cellcolor{CornflowerBlue!75}{4.02\%} &
	 \cellcolor{CornflowerBlue!75}{3.73\%} &
	 \cellcolor{CornflowerBlue!75}{3.22\%} &
	 \cellcolor{CornflowerBlue!75}{4.41\%} &
	 \cellcolor{CornflowerBlue!75}{3.80\%} &
	 \cellcolor{CornflowerBlue!75}{3.59\%} &
	3.22\% &
	 \cellcolor{CornflowerBlue!50}{4.33\%} \\
	\symSDT~Haar &
	4.12\% &
	 \cellcolor{CornflowerBlue!50}{3.85\%} &
	3.41\% &
	4.65\% &
	3.95\% &
	3.82\% &
	3.22\% &
	4.58\% \\
	\symFIR~DLOP &
	 \cellcolor{CornflowerBlue!25}{4.04\%} &
	3.90\% &
	 \cellcolor{CornflowerBlue!50}{3.30\%} &
	 \cellcolor{CornflowerBlue!50}{4.46\%} &
	 \cellcolor{CornflowerBlue!25}{3.86\%} &
	 \cellcolor{CornflowerBlue!25}{3.65\%} &
	 \cellcolor{CornflowerBlue!25}{3.16\%} &
	 \cellcolor{CornflowerBlue!75}{4.32\%} \\
	\symFIR~Random &
	5.38\% &
	5.12\% &
	4.38\% &
	6.15\% &
	4.04\% &
	3.87\% &
	3.30\% &
	4.51\% \\
	\symFIR~Identity &
	5.10\% &
	4.82\% &
	4.31\% &
	5.89\% &
	4.04\% &
	3.87\% &
	3.31\% &
	4.67\% \\
	\bottomrule
	\end{tabular}\\[10pt]
	\centering\small\sffamily
	\setlength{\tabcolsep}{4.75pt}
	\begin{tabular}{r r  c c c c c c c c  c c c c c c c c}
	\toprule
	& & \multicolumn{8}{c}{{\color{skyblue1}$\blacksquare$} \textbf{Fixed convolution}}
	& \multicolumn{8}{c}{{\color{aluminium2}$\blacksquare$} \textbf{Learned convolution}} \\
	\cmidrule(r){3-10}\cmidrule(l){11-18}
	\emph{Basis} & & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8)  & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) \\
	\midrule
	\symLTI~LDN & (1) &
	 &
	\sigB &
	 &
	 &
	 &
	 &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symLTI~Mod.~Fourier & (2) &
	\sigB &
	 &
	 &
	\sigC &
	 &
	\sigB &
	\sigC &
	\sigB &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symSDT~Fourier & (3) &
	 &
	 &
	 &
	 &
	 &
	 &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symSDT~Cosine & (4) &
	 &
	\sigC &
	 &
	 &
	 &
	 &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symSDT~Haar & (5) &
	 &
	 &
	 &
	 &
	 &
	 &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symFIR~DLOP & (6) &
	 &
	\sigB &
	 &
	 &
	 &
	 &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symFIR~Random & (7) &
	\sigC &
	\sigC &
	\sigC &
	\sigC &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\symFIR~Identity & (8) &
	\sigC &
	\sigB &
	\sigC &
	\sigC &
	\sigC &
	\sigC &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 &
	 \\
	\bottomrule
	\end{tabular}
\end{table}
