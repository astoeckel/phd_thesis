% !TeX spellcheck = en_GB

\section{Summary}
\label{sec:nlif_discussion}

In this chapter, we discussed dendritic computation from the perspective of function approximation.
Specifically, we assumed that neurons with dendritic structures can be modelled as multivariate nonlinearities $\sigma(\xi_1, \ldots, \xi_k)$, where each $\xi_i$ constitutes a separate input channel.
While such multivariate functions cannot be used as universal approximators, we argued that they can sometimes outperform resource-constrained two-layer networks.

To test whether this hypothesis holds in functional spiking neural networks, we extended the Neural Engineering Framework (\NEF) to account for additional constraints required to build networks with biologically plausible multi-channel neurons.
Specifically, we presented techniques for solving for synaptic weights in current-space while complying with Dale's principle.
We further introduced \enquote{subthreshold relaxation}, an extension to the weight optimisation problem that exploits the rectifier nature of biological neurons to improve the superthreshold current-approximation accuracy.
Finally, we discussed a general framework for incorporating multi-channel neurons into the \NEF using a dendritic nonlinearity \Hden.

We continued by describing the \nlif family of multi-compartment neurons, a model of spiking neurons with passive dendrites.
While it is not possible to state a closed form dendritic nonlinearity model \Hden that characterises these neurons, we derived a parametrised surrogate model by harnessing the equilibrium state of the \nlif dynamical system.
We systematically characterised the computational properties of the surrogate model in terms of product terms and divisive interaction between input channels.

A particular focus of our studies was on the \twocomplif neuron, the simplest, non-trivial \nlif model.
This model features two conductance-based input channels with divisive interaction.
Notably, we can solve for synaptic weights in \twocomplif neurons by minimising a loss function in rational least-squares form, which in turn can be approximated as a convex linear least-squares problem.
Spiking neural networks with two-compartment neurons outperform two-layer networks for a variety of benchmark functions, as long as these functions do not implicitly require solving the \XOR problem.

Solving for surrogate model parameters and synaptic weights in arbitrary \nlif neurons is a hard nonconvex optimisation problem.
We demonstrated that it is possible to exploit the structure of the surrogate dendritic nonlinearity by constructing a sequential quadratic program (\SQP) that reaches relatively small losses after a few iterations and that often outperforms gradient-based methods such as L-\BFGS-B and Adam.

The methods presented here can approximate \XOR-like functions such as four-quadrant multiplication using \nlif neurons with three or more compartments.
Furthermore, these neurons can be integrated into spiking neural networks.
Despite numerous simplifying assumptions in the derivation of our surrogate model, they typically outperform two-layer networks.
However, the improvement in function approximation accuracy compared to two-compartment neurons is limited by the surrogate dendritic nonlinearity becoming less accurate the more compartments are in the neuron.

\subsubsection{Limitations of our work}
While the work presented in this chapter is a step towards a general model of dendritic computation in functional neurobiological modelling frameworks, it has several limitations.
Most importantly, we treat the dendritic nonlinearity \Hden as time-independent.
Correspondingly, we implicitly assume that synaptic dynamics typically dominate the overall neuronal dynamics.
However, dendritic trees in biology---especially when considering active channels and dendritic spikes \citep{koch1999biophysics,koch2002singlecell}---possess filter properties and adaptation processes that are not accounted for in our model.
We discuss techniques that could---in the future---exploit these dynamics in the next chapter.

Another shortcoming of our surrogate model \Hden is the assumption that the average somatic membrane potential is constant.
While we accounted for this by calibrating the model parameters, this calibration is specific to the working-regime of the neuron.
Deviations from the modelled behaviour are particularly apparent in situations with small output rates (cf.~\Cref{fig:avg_vsom,fig:synaptic_nonlinearity_fit_a,fig:synaptic_nonlinearity_fit_b,fig:nlif_parameter_optimisation_comparison}).
Correspondingly, our surrogate dendritic nonlinearity may not be a suitable model for brain areas featuring extremely low maximum firing rates.
Given that optimising for weights in arbitrary \nlif neurons is a \enquote{hopeless} nonconvex optimisation problem anyway, it would be interesting to consider the use of a less constrained statistical model as a dendritic nonlinearity \Hden, such as a small neural network.

In light of these limitations, we would like to re-emphasize that, as stated in the introduction of this chapter, our goal was not to provide a detailed mechanistic model of dendritic computation.
Instead, we hope to provide a useful tool that captures essential aspects of dendritic computation---a nonlinear interaction between input channels---while being computationally cheap and mathematically tractable, but still grounded in biophysics.

In particular, we saw that both the two- and three-compartment neuron improve the multivariate function approximation performance of a single neuron population, while using more than three compartments resulted in diminishing returns.
From a purely functional perspective, there thus seems to be no reason to use more complex models of \emph{passive} dendritic computation beyond two and three-compartment neurons.

\subsubsection{Future work}
A potential application of our work outside neurobiological modelling is programming neuromorphic hardware (\Cref{sec:neuromorphic}).
Especially when considering mixed analogue-digital neuromorphic hardware, it should be possible to achieve a higher energy efficiency by implementing two- or three-compartment neuron models and performing local analog computation.
Potential future work would be to validate our methods on a neuromorphic platform that implements dendritic trees, such as the \emph{BrainScales 2} system \citep{schemmel2017accelerated}, or analogue circuit emulators such as FPAAs \citep[e.g.,][]{george2016programmable}.

Other open threads that warrant further investigation include ruling out that the diminishing returns for three- and four-compartment neurons are not just due to our weight solver converging to suboptimal solutions compared to other solvers.
It could also be interesting to find ways to improve the prediction of the $x$-intercept in our surrogate dendritic nonlinearity, and to further strengthen the mathematical foundation of the material in this chapter.
