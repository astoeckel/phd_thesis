% !TeX spellcheck = en_GB

\section{Networks of Two-Compartment LIF Neurons}
\label{sec:two_comp_lif}

Up to this point, we discussed the theoretical advantages of dendritic computation, extended the Neural Engineering Framework to support more complex connectivity constraints and neuron types, and introduced and analysed a family of multi-compartment \LIF neurons with passive dendritic trees that we called \enquote{\nlif} neurons.

The goal of this section is to systematically incorporate the simplest non-trivial \nlif neuron into \NEF networks---namely the two-compartment \LIF neuron depicted in \Cref{fig:nlif_c}.
This represents the smallest possible step toward exploiting dendritic nonlinearities, and as such is a good test for our overall methodology.

We proceed as follows.
First, we discuss a method for estimating the parameters of our surrogate dendritic nonlinearity model $H$ using non-negative least-squares and test the quality of the estimated parameters in various scenarios.
A similar approach can be used derive a \qprog for the synaptic weights that takes nonnegative connectivity and subthreshold relaxation into account.
We use this approach to evaluate the computational advantage of the two-compartment \LIF nonlinearity in an optimal scenario without dynamics, akin to our experiment in \Cref{sec:dendritic_computation_theory_numerical}.
Finally, we demonstrate that this theoretical advantage persists in a spiking neural network context.

\subsection{Estimating Model Parameters}
\label{sec:two_comp_lif_fit_model}

We presented the parametrised dendritic nonlinearity surrogate model for two-compartment \LIF neurons in \cref{eqn:two_comp_lif}.
While the model parameters $a_0$, $a_1$, $a_2$, $b_0$, $b_1$, $b_2$ can be estimated using \cref{eqn:two_comp_lif_natural}, it is better to fit the parameters to data from numerical simulations.

One way to generate these data is to simulate the spiking neuron for different input conductances $g_{\mathrm{E}, k}$, $g_{\mathrm{I}, k}$ over a period of time $T$ and to compute the average spike rate $a_k = \mathscr{G}( g_{\mathrm{E}, k}, g_{\mathrm{I}, k})$.
As discussed in \Cref{sec:nef_nonlinear}, we then obtain the somatic current $J_k$ by applying the inverse of the chosen one-dimensional response curve $G^{-1}$ to $a_k$.
%From our experience, this method yields results superior to measuring $J_k$ directly in the simulation.
Importantly, when doing this, samples with small $a_k$ should be ignored: $G^{-1}$ is not well-defined for zero rates, and $H$ was derived for superthreshold dynamics.

\subsubsection{Optimisation problem}
Given the superthreshold samples $J_k$, $g_{\mathrm{E}, k}$, $g_{\mathrm{I}, k}$ we now have the following least-squares loss function over the parameters $a_i$, $b_i$:
\begin{align}
	E &=
		\sum_{k = 1}^N \bigl( J_k - H(g_\mathrm{E, k}, g_\mathrm{I, k}) \bigr)^2
	= \sum_{k = 1}^N \left( \! J_k - \frac{b_0 + b_1 g_{\mathrm{E}, k} - b_2 g_{\mathrm{E}, k}}{a_0 + a_1 g_{\mathrm{E}, k} + a_2  g_{\mathrm{I}, k}} \right)^2 \,,
	\;\; \begin{aligned}\text{subject to } a_0 &> 0 \,, \\ \text{and } a_1, a_2, b_0, b_1, b_2 &\geq  0 \,.\end{aligned}
	\label{eqn:two_comp_optimal_parameters}
\end{align}
Note that this optimisation problem has one superfluous degree of freedom.
A numerically stable normalisation is to set $b_1 = 1$.
In this case, all parameters are expressed relative to the effect of the excitatory channel on the input current.%
\footnote{As we mentioned in a footnote above, this is a result of the the scale-invariance of the individual rows of $\mat{\tilde A}$ and $\vec{\tilde b}$. In general, a parameterised \nlif neuron has $n - 1$ superfluous degrees of freedom in its parameters.}

\begin{figure}
	\centering
	\includegraphics{media/chapters/03_nlif/03_04/two_comp_lif_loss_comparison.pdf}
	\caption[Comparison between the actual and substitute loss function.]{Comparison between the actual loss function (eq.~\ref{eqn:two_comp_optimal_parameters}; \textbf{A}) and the substitute loss function (eq.~\ref{eqn:two_comp_optimal_parameters_prime}; \textbf{B}).
	Each plot depicts a slice of the error function $E$ or $E'$ over two parameters.
	Black crosses correspond to the optimal solution with respect to the substitute loss function $E'$, black circles correspond to the closest local optimum in the actual loss function.
	The underlying data is sampled from a two-compartment neuron with  $E_\mathrm{E} = \SI{20}{\milli\volt}$, $E_\mathrm{I} = \SI{-75}{\milli\volt}$, $E_\mathrm{L} = \SI{-65}{\milli\volt}$, $g_\mathrm{L} = \SI{50}{\nano\siemens}$, $c_{12} =\SI{30}{\nano\siemens}$ for $N = 318$ superthreshold samples of $\gE$, $\gI$ over $[\SI{0}{\nano\siemens}, \SI{500}{\nano\siemens}]$.
	The \RMSE current error for the optimised weights is $\sqrt{E/N} = \SI{15.22}{\pico\ampere}$ when using the actual loss function for optimisation, and $\sqrt{E/N} = \SI{15.67}{\pico\ampere}$ when using the substitute loss function for optimisation ($3\%$ increase). The parameter $b_1$ is set to one.
	}
	\label{fig:two_comp_lif_loss_comparison}
\end{figure}

Unfortunately, \cref{eqn:two_comp_optimal_parameters} is neither in \emph{linear} least-squares form, nor convex.
%(see \Cref{app:two_comp_lif_non_convex}).
In theory, this complicates solving for model parameters \citep{rockafellar1993lagrange}.
Fortunately, we can largely work around this by minimising a convex substitute loss function (cf.~\Cref{fig:two_comp_lif_loss_comparison}).
Optimally, we would like the following equality to hold for each sample $k$
\begin{align*}
	J_k &= H(g_{\mathrm{E}, k}, g_{\mathrm{I}, k}) = \frac{
		b_0 + b_1 g_{\mathrm{E}, k} - b_2 g_{\mathrm{I}, k}
	}{
		a_0 + a_1 g_{\mathrm{E}, k} + a_2 g_{\mathrm{I}, k}
 	} \Leftrightarrow 0 = J_k (a_0 + a_1 g_{\mathrm{E}, k} + a_2 g_{\mathrm{I}, k}) - b_0 - b_1 g_{\mathrm{E}, k} + b_2 g_{\mathrm{I}, k} \,.
\end{align*}
Notably, the two equations are equivalent because the denominator is strictly non-zero.
Phrasing this as a loss function for multiple samples yields
\begin{align}
	E' &= \sum_{k = 1}^N \bigl( J_k (a_0 + a_1 g_{\mathrm{E}, k} + a_2 g_{\mathrm{I}, k}) - b_0 - b_1 g_{\mathrm{E}, k} + b_2 g_{\mathrm{I}, k} \bigr)^2 \,,
	\label{eqn:two_comp_optimal_parameters_prime}
\end{align}
subject to the same constraints as above.
Arranging the samples in vectors $\vec J$, $\vec g_\mathrm{E}$, $\vec g_\mathrm{I} \in \mathbb{R}^N$, we can bring this into a canonical matrix form (where \enquote{$\circ$} is elementwise multiplication):
\begin{align}
	E' &= \|\mat A \vec \omega - \vec b \|_2^2 \,, & \text{where } \mat A = (\vec J, \vec J \, \circ \,  \vec g_\mathrm{E}, \vec J \, \circ \, \vec g_\mathrm{I}, -\vec{1}, \vec{g}_\mathrm{I}) \,, \; \vec b = \vec{g}_\mathrm{E} \,, \; \vec \omega &= (a_0, a_1, a_2, b_0, b_2) \,.
	\label{eqn:two_comp_optimal_parameters_prime_matrix}
\end{align}

Crucially, and as is illustrated in \Cref{fig:two_comp_lif_loss_comparison}, the solutions obtained by minimising \cref{eqn:two_comp_optimal_parameters,eqn:two_comp_optimal_parameters_prime} are similar, but not equivalent.
While both optimisation problems minimise the error for each sample, multiplication with the denominator in \cref{eqn:two_comp_optimal_parameters_prime} causes samples to be dynamically re-weighted.
In our application, the impact of this is fairly low in practice, and the global minimum of the substitute loss seems to be close to a minimum in the actual loss function.
This method has been proposed in the context of fitting transfer functions by \citet{levy1959complexcurve}.

\subsubsection{Parameter refinement}
If an exact solution in terms of the closest local optimum of \cref{eqn:two_comp_optimal_parameters} is desired, the parameters can easily be refined using gradient descent.
Alternatively, and as originally suggested by \citet{sanathanan1963transfer} in the context of fitting transfer functions, we can refine the solution by iterative reweighting of \cref{eqn:two_comp_optimal_parameters_prime_matrix}.
Each sample is weighted by the inverse of its denominator from the previous iteration, i.e.,
\begin{align}
	E' &= \|\diag(\vec w)^{-1} \mat A \vec \omega - \diag(\vec w)^{-1}\vec b \|_2^2 \,, & \text{where } \vec w = a_0 \vec{1} + a_1 \vec g_\mathrm{E} + a_2 \vec g_\mathrm{I} \,.
	\label{eqn:sanathanan_koerner}
\end{align}
This scheme converges to a (not necessarily globally optimal) fixed point.
In our application this seems to be the case after two additional iterations.
A more thorough review of this \enquote{Sanathanan-Koerner iteration} is given in \citet[Section~2.2]{hokanson2018least} and \citet{pintelon1994parametric}.
For the sake of simplicity, and because we do not observe a significant reduction in errors, we chose not to rely on this parameter refinement in our experiments.

\subsection{Experiment 1: Precision of the Model Parameter Estimation}
\label{sec:two_comp_lif_experiment_1}

The following experiment tests in how far the surrogate model $G[H(\gE, \gI)]$ can be used to accurately predict the spike rate $\mathscr{G}(\gE, \gI)$ of a simulated spiking two-compartment \LIF neuron---both before and after fitting the model parameters to empirical data.
We conduct this experiment in an artificial scenario with constant conductances \gE, \gI, and in a simulated network context with artificial temporal spike noise superimposed onto the inputs.

\begin{figure}[t]
	\includegraphics{media/chapters/03_nlif/03_04/model_parameter_fits_a.pdf}%
	{\phantomsubcaption\label{fig:synaptic_nonlinearity_fit_a_a}}%
	{\phantomsubcaption\label{fig:synaptic_nonlinearity_fit_a_b}}%
	\caption[Two-compartment nonlinearity model for constant input]{
		Two-compartment nonlinearity model for constant input. Contour plots depict measured average spike rates $\mathscr{G}(g_\mathrm{E}, g_\mathrm{I})$.
		Dashed lines are the model prediction $G[H(g_\mathrm{E}, g_\mathrm{I})]$. Dotted lines indicate the cross-section location.
		$E$ denotes the spike-rate \RMSE.
		Columns correspond to different coupling conductances $c_\mathrm{12}$.
		The model fits the simulation well, with minor deviations near the spike onset.}
	\label{fig:synaptic_nonlinearity_fit_a}%
\end{figure}

\subsubsection{Experiment 1.1: Constant conductances}
We analyse three two-compartment \LIF neurons with coupling conductances $c_{12} = \SI{50}{\nano\siemens}$, $\SI{100}{\nano\siemens}$, and $\SI{200}{\nano\siemens}$; all other parameters are listed in \Cref{tbl:two_comp_neuron_parameters}.
We measure the output spike rate for constant input conductances $g_\mathrm{E}$, $g_\mathrm{I}$ on a $100 \times 100$ grid.
For each grid-point, the neuron's dynamical system (cf.~\Cref{sec:nlif_description}) is simulated for $T = \SI{1}{\second}$.
We then compute the steady-state firing rate by taking the inverse of the median inter-spike-interval.
The conductance range has been selected such that the maximum rate is \SI{100}{\per\second}, and the spike onset coincides with the diagonal of the \gE-\gI-rate contour plot.

We compare these numerical simulation results to both the theoretical somatic current prediction according to \cref{eqn:two_comp_lif_natural}, and the parameterised dendritic nonlinearity with the parameters fitted to the measured data.
Parameter optimisation according to \cref{eqn:two_comp_optimal_parameters_prime_matrix} is based on a training-set of \num{200} conductance pairs sampled with uniform probability from the conductance-range.
The final prediction error is computed over all \num{10000} grid points.

\pagebreak

\paragraph{Results}
The results for this experiment are depicted in \Cref{fig:synaptic_nonlinearity_fit_a}; the fitted parameters can be found in \Cref{tbl:two_comp_model_parameters}.
Unsurprisingly, when using the theoretical parameter estimate from \cref{eqn:two_comp_lif_natural}, there is a substantial discrepancy between the model prediction and the simulation, especially for large $c_{12}$ (\Cref{fig:synaptic_nonlinearity_fit_a_a}). This discrepancy is reduced after fitting the model parameters (\Cref{fig:synaptic_nonlinearity_fit_a_b}).
The model prediction fits the empirical data well for output spike rates greater than \SI{25}{\per\second}.
However, it fails to predict the spike onset correctly, placing it too early with respect to increasing \gE.
As we predicted in \Cref{sec:nlif_theory}, the linearity of $H$ increases as $c_{12}$ is increased, i.e., the contour lines become more \enquote{parallel} for larger $c_{12}$.

\paragraph{Discussion}
Overall, the model works well once the parameters have been fitted.
The mismatch between measured rates and the theoretical model for large $c_{12}$ is likely due to the more pronounced effect of the somatic superthreshold dynamics on the dendritic compartment.
Failure to predict the spike onset is likely due to our model not capturing low firing-rate regimes of the neuron well.

\subsubsection{Experiment 1.2: Conductances with artificial temporal spike noise}

\begin{figure}[t]
	\includegraphics{media/chapters/03_nlif/03_04/model_parameter_fits_b.pdf}%
	\caption[Two-compartment nonlinearity model for noisy input]{Two-compartment nonlinearity model for noisy input with combined excitatory Poisson spike rates of $1/\lambda = \SI{4500}{\per\second}$ for excitatory synapses and $1/\lambda = \SI{1800}{\per\second}$ for inhibitory synapses.
	See \Cref{fig:synaptic_nonlinearity_fit_a} for a complete legend.
	The model fits the noisy data better than the noise-free data.
	    }
	\label{fig:synaptic_nonlinearity_fit_b}%
\end{figure}

In a network context, \gE and \gI are not constant, but, as we discussed in Section~2.2.2, a weighted sum of low-pass filtered spike trains.
This results in a considerable amount of \enquote{spike noise} in the conductance inputs.
In this experiment, we generate artificial filtered spike trains using spike times from Poisson sources (simulation period $T = \SI{100}{\second}$; rates fitted to data from Experiment~3; see below).
Synaptic weights are simulated by uniformly sampling scaling factors for each spike event.
The resulting signals are scaled such that the averages conductances are equal to \gE, \gI.
We de
We furthermore replace $G[J]$ with a ReLU, that is $G[J] = \max\{ 0, \alpha J + \beta \}$.
We know from preliminary experiments that the hard \LIF spike-onset is not present in noisy environments---this is not captured well by the standard \LIF response curve.
While we could use a \enquote{soft} version of the \LIF response curve that takes this into account \citep[cf.][]{capocelli1971diffusion,hunsberger2014competing,kreutz2015mean}, a \ReLU appears to work well.

\paragraph{Results}
The results of this experiment are depicted in \Cref{fig:synaptic_nonlinearity_fit_b}.
Final fitted parameters are provided in \Cref{tbl:two_comp_model_parameters}.
Surprisingly, our model fits the noisy spike rates better than non-noisy data.
Still, the overall trends from the previous experiment are still visible.
When using the theoretical parameter estimates, larger $c_{12}$ result in higher overall errors and fitting the model parameters greatly reduces the error to values of about \SIrange{1}{2}{\per\second}.
While the sharp spike onset is no longer present, the model does not capture the subtle sigmoid shape of the response curve near the spike onset that is particularly pronounced for larger $g_\mathrm{C}$.

\paragraph{Discussion}
To summarise, our results indicate that, after fitting the model parameters, the surrogate model $H$ can indeed be used to predict the neural response curve $\mathscr{G}(\gE, \gI)$ with a relatively high accuracy in both tested scenarios.
It seems reasonable to choose a different one-dimensional neural response curve $G[J]$ when taking noise in the input into account.


\subsection{Solving for Synaptic Weights}
\label{sec:two_comp_synaptic_weights}

We demonstrated that the surrogate model $H(\gE, \gI)$ can be used to quite accurately predict the firing rates of individual two-compartment neurons.
Now, to construct \NEF networks, we must find weights $\vec w^\mathrm{E}_i$, $\vec w^\mathrm{I}_i$ that fulfil the normative tuning-curve constraint from \cref{eqn:dendritic}.

\subsubsection{Optimisation problem}
Given a function $f(\vec x)$ that we would like to compute, we optimally minimise the following least-squares loss over $\vec w^\mathrm{E}_i$, $\vec w^\mathrm{I}_i$ for each post-neuron $i$
\begin{align}
	E &= \frac{1}{\Xrepr} \int_{\Xrepr} \mathcal{E} \left( J_i(f(\vec x)), \frac{
		b_0 + b_1 \langle \vec w^\mathrm{E}_i, \vec a(\vec x) \rangle - b_2 \langle \vec w^\mathrm{I}_i, \vec a(\vec x) \rangle
	}{
		a_0 + a_1 \langle \vec w^\mathrm{E}_i, \vec a(\vec x) \rangle + a_2 \langle \vec w^\mathrm{I}_i, \vec a(\vec x) \rangle
 	}	
	\right)^2 \, d\vec x + \sigma^2 \| \vec w^\mathrm{E}_i \|^2_2 + \sigma^2 \| \vec w^\mathrm{I}_i \|^2_2 \,,
	\label{eqn:two_comp_optimal_weights}
\end{align}
subject to $\vec w^\mathrm{E}_i$, $\vec w^\mathrm{I}_i \geq 0$,
and where $\mathcal{E}$ is the superthreshold error function (eq.~\ref{eqn:subthreshold_error}), $\vec a(\vec x)$ are the stacked pre-activities of all pre-populations, and $J_i(\vec x)$ is the current-translation function.

Just like the model parameter optimisation problem from the previous subsection, \cref{eqn:two_comp_optimal_weights} is not convex.
%(again, see \Cref{app:two_comp_lif_non_convex} for more details).
Again, we work around this by multiplying with the denominator inside the square to obtain a substitute loss function:
\begin{align}
	\begin{aligned}
	E' &=
		\int_{\Xrepr} \mathcal{E}
		\Bigl(
			J_i(f(\vec x)) (a_0 + a_1 \langle \vec w^\mathrm{E}_i, \vec a(\vec x) \rangle + a_2 \langle \vec w^\mathrm{I}_i, \vec a(\vec x) \rangle )\,, \\[-1em]
		&\hspace{3.66em}		
		b_0 + b_1 \langle \vec w^\mathrm{E}_i, \vec a(\vec x) \rangle - b_2 \langle \vec w^\mathrm{I}_i, \vec a(\vec x) \rangle
		\Bigr)^2 \, d\vec x
		 + \sigma^2 \| \vec w^\mathrm{E}_i \|^2_2 + \sigma^2 \| \vec w^\mathrm{I}_i \|^2_2 \,,
	\end{aligned}
	\label{eqn:two_comp_optimal_weights_prime}
\end{align}
subject to $\vec w^\mathrm{E}_i$, $\vec w^\mathrm{I}_i \geq 0$.
A sampled version of this problem can be phrased as a \qprog.
To see this, let $\mat A \in \mathbb{R}^{N \times n}$ be a matrix of pre-activities and $\vec J_i \in \mathbb{R}^{N}$ be a vector of target currents.
Assume that we only have superthreshold samples, that is $\mathcal{E}(J_\mathrm{tar}, J_\mathrm{dec}) = J_\mathrm{tar} - J_\mathrm{dec}$. We have:
\begin{align}
	E' &\propto \bigl\|	\,
		  \vec J_i \circ
		  	\bigl(
		  		a_0 +
		  		a_1 \mat{A} \vec w_i^\mathrm{E} +
		  		a_2 \mat{A} \vec w_i^\mathrm{E} \bigr)
	      - \bigl(
	      		b_0 +
	      		b_1 \mat{A} \vec w_i^\mathrm{E} -
	      		b_2 \mat{A} \vec w_i^\mathrm{I} \bigr)
	\, \bigr\|_2^2
	+ \sigma^2 \bigl\| \vec w^\mathrm{E}_i \bigr\|^2_2 + \sigma^2 \bigl\| \vec w^\mathrm{I}_i \bigr\|^2_2 \notag \\
		&= \hspace{0.125em} \bigl\|
			\bigl(a_1 \diag(\vec J_i) \mat A - b_1 \mat A \bigr) \vec w_i^\mathrm{E}
			+ \bigl(a_2 \diag(\vec J_i) \mat A + b_2 \mat A \bigr) \vec w_i^\mathrm{I}
			+ (a_0 - b_0) \vec J_i
	\, \bigr\|_2^2
	+ \sigma^2 \bigl\| \vec w^\mathrm{E}_i \bigr\|^2_2 
	+ \sigma^2 \bigl\| \vec w^\mathrm{I}_i \bigr\|^2_2  \notag \\
	&= \hspace{0.125em} \bigl\|
		\mat A' \vec w_i + \vec J_i'
	\bigr\|_2^2
	+ \sigma^2 \bigl\| \vec w_i \bigr\|^2_2  \,.
	\label{eqn:two_comp_optimal_weights_prime_matrix}
\end{align}
To account for subthreshold relaxation, we simply split $\mat A'$ and $\vec J'$ into super- and subthreshold samples as discussed in \Cref{sec:nef_subthreshold} and use the QP defined in \cref{eqn:decode_subthreshold_qp}.

Again, we could use the Sanathanan-Koerner iteration (cf.~eq.~\ref{eqn:sanathanan_koerner}) to refine the solution.
However, in our experience, and as with the parameter optimisation problem, the solution to the convex problem tends to be close to a local optimum in the original rational loss function.

\subsubsection{Examples: Addition and gain modulation}
Although we analyse two-compartment neurons more thoroughly in the next two sections, we would first like to test our weight optimisation scheme in the context of two specific functions $f$: addition and nonnegative multiplication.

Specifically, we use the two-compartment \LIF neuron with $c_{12} = \SI{50}{\nano\siemens}$ (parameter set (ii) from \Cref{tbl:two_comp_neuron_parameters})
and the network setup discussed in
\Cref{sec:dendritic_computation_theory_dendritic}.
That is, two pre-populations with 200 neurons each project onto a single post-neuron; the pre-populations represent $x_1$, $x_2$, respectively, while the post-neuron represents $\hat y \approx f(x_1, x_2)$.%
\footnote{While an individual neuron is not sufficient for reconstructing the represented value $\hat y$ via linear decoding, we can infer $\hat y$ by inverting the current translation function; that is $\hat y = J^{-1}[H(\gE(x_1) + \gE(x_2), \gI(x_1) + \gI(x_2))]$.
Our post-neuron has a positive encoder, an $x$-intercept of zero, and a maximum rate of \SI{100}{\per\second}.
}
As depicted in \Cref{fig:nef_multivariate_functions_c}, the synaptic weights implicitly define a set of conductance functions.
Due to commutativity of addition and multiplication, the excitatory and inhibitory functions decoded from each pre-population optimally are the same; we have $\gE(x) = g_\mathrm{E}^1(x) = g_\mathrm{E}^2(x)$ and $\gI(x) = g_\mathrm{I}^1(x) = g_\mathrm{I}^2(x)$.
We emulate current-based \LIF neurons by setting $a_0 = b_1 = 1$, $b_2 = -1$, and $a_1 = a_2 = b_0 = 0$ in the nonlinearity model $H$.
For these parameters we have $H(J_\mathrm{E}, J_\mathrm{I}) = J_\mathrm{E} - J_\mathrm{I}$.
%while the synaptic weights implicitly describe excitatory and inhibitory current functions $J_\mathrm{E}(x)$ and $J_\mathrm{I}(x)$.

\begin{figure}
	\centering
	\includegraphics{media/chapters/03_nlif/03_04/two_comp_weights_examples_addition.pdf}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_addition_a}}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_addition_b}}%
	\caption[Computing addition in single- and two-compartment neurons]{Computing addition in \textbf{(A)} single- and \textbf{(B)} two-compartment neurons. See text for a detailed description. \emph{Top:} Decoded current and conductance functions. \emph{Left:} Decoded represented value. Black contour lines and coloured backdrop correspond to the decoded values $\hat y$; dashed white lines to the target function $y$. \emph{Right:} Decoding error $\hat y - y$. Depicted error value $E$ is the \NRMSE.
	Single- and two-compartment neurons are equally suitable for computing linear functions.
	}
	\label{fig:two_comp_weights_examples_addition}
\end{figure}

%\paragraph{Results: Addition}
Results for computing addition, that is $f(x_1, x_2) = \frac{1}2 (x_1 + x_2)$ over $(x_1, x_2) \in [0, 1]^2$, are depicted in \Cref{fig:two_comp_weights_examples_addition}.
As is expected, we can compute this function with a low \NRMSE with the current-based nonlinearity.
Perhaps unintuitively, and in spite of the dendritic nonlinearity $H$, we achieve similarly low errors using the two-compartment \LIF neuron.

The way in which the weight solver accomplishes this becomes apparent when considering the decoded current functions \gE and \gI in \Cref{fig:two_comp_weights_examples_addition_b}.
Both $\gE$ and $\gI$ are affine functions with opposing slopes.
Correspondingly, the denominator $a_0 + a_1 \gE + a_2 \gI$ stays approximately constant, while the numerator $b_0 + b_1 \gE - b_2 \gI$ generates the desired target currents.


\begin{figure}
	\centering
	\includegraphics{media/chapters/03_nlif/03_04/two_comp_weights_examples_multiplication.pdf}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_multiplication_a}}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_multiplication_b}}%
	\caption[Computing multiplication in single- and two-compartment neurons]{Computing multiplication in \textbf{(A)} single- and \textbf{(B)} two-compartment neurons. See text for a detailed description and \Cref{fig:two_comp_weights_examples_addition} for a legend. Nonnegative multiplication can be reasonably approximated using two-compartment neurons. Dotted line in \emph{(B)} is a hyperbolic fit.
	}
	\label{fig:two_comp_weights_examples_multiplication}
\end{figure}

%\paragraph{Results: Multiplication}
Results for computing nonnegative multiplication, that is $f(x_1, x_2) = x_1 x_2$ over $(x_1, x_2) \in [0, 1]^2$, are depicted in \Cref{fig:two_comp_weights_examples_multiplication}.
Using current-based \LIF neurons we can, at best, approximate multiplication with a linear function. This results in an \NRMSE of about 25\%.
In contrast, using the two-compartment \LIF nonlinearity results in an error of about 6\%, with the largest discrepancies being in regions are both $x_1$ and $x_2$ are close to one.


\begin{figure}
	\centering
	\includegraphics{media/chapters/03_nlif/03_04/two_comp_weights_examples_statistics.pdf}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_statistics_a}}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_statistics_b}}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_statistics_c}}%
	{\phantomsubcaption\label{fig:two_comp_weights_examples_statistics_d}}%
	\caption[Error and weight statistics for computing addition and nonnegative multiplication]{Error and weight statistics for computing addition and nonnegative multiplication. \textbf{(A, B)}
	
	Depicted is the median over $1000$ experiments, shaded areas the 25/75-percentiles.
	While errors monotonically decreases with more pre-neurons in the addition task \emph{(A)}, errors quickly plateau when computing multiplication \emph{(B)}.
	\textbf{(C,~D)}~Comparison between the weight magnitude frequencies for the two tasks and neuron types (for $n = 300$ pre-neurons).
	Weights are normalised such that a value of one corresponds to $\SI{1}{\nano\ampere}$ or $\SI{1}{\nano\siemens}$, respectively.
	Dashed line and depicted values are the median; frequencies include zero-weights. Weights below $10^{-6}$ are counted as zero. Two-compartment neurons require larger weight magnitudes. The spread of the magnitudes changes depending on the computed function.
	}
	\label{fig:two_comp_weights_examples_statistics}
\end{figure}

Importantly, and as is depicted in \Cref{fig:two_comp_weights_examples_statistics_b}, two-compartment \LIF neurons cannot approximate nonnegative multiplication with an arbitrarily small error; more precisely, the error cannot be reduced past 6\% for a single post-neuron.
This is in contrast to addition, where we can reach arbitrarily small errors by increasing the number of pre-neurons (cf.~\Cref{fig:two_comp_weights_examples_statistics_a})
It is possible to reach smaller (but not arbitrarily small) errors with multiple post neurons dividing up the represented space (we reach down to $4\%$; see $E_\mathrm{model}$ in \Cref{tbl:function_approximations_complete}).

Curiously, looking at the conductance functions \gE and \gI, \emph{both} excitation and inhibition increase substantially for small $x_1$ or $x_2$, similar to a shifted and scaled hyperbola  $(\beta + \alpha x)^{-1}$ (black dotted line in \Cref{fig:two_comp_weights_examples_multiplication_b}).
This may be counter-intuitive, given that inhibition is typically responsible for shutting off the target neuron.%
\footnote{Remember that these results are for a single post-neuron with positive encoder. That is, to represent smaller values, the neuron must receive a smaller input current.}
However, when increasing both excitation and inhibition, this common-mode increase mostly cancels out in the numerator (due to the inhibitory conductance being subtracted), but increases the magnitude of the denominator; this amplifies the divisive effect of the input.

Notably, as we mentioned in the introduction of this chapter, nonnegative multiplication (or, alternatively, nonnegative division) is also referred to as \enquote{gain modulation} in the neuroscience literature \citep{salinas2000gain}.
The observation that both excitation and inhibition must be increased to multiplicatively (or divisively) reduce the gain of a neuron is consistent with \emph{in vitro} experiments \citep{chance2002gain}.

An earlier hypothesised mechanism for gain modulation is \emph{shunting inhibition}.
Here, the idea is that an inhibitory channel with a reversal potential close to the resting potential acts divisively on the average input current.
This effect could in theory be used to implement multiplication \citep{koch1992multiplying}; however, in practice, increasing inhibitory conductances alone has a predominantly linear effect on the spike rate (since $a_2 \ll b_2$), or requires implausibly high conductance values \citep{holt1997shunting,abbott2005drivers}.

Even when exploiting the common increase of excitation and inhibition, generating the large input conductances for small $x_1$, $x_2$ results in relative large synaptic weights.
This is depicted in \Cref{fig:two_comp_weights_examples_statistics_c,fig:two_comp_weights_examples_statistics_d}.
Compared to computing addition, the median synaptic weight increases by a factor of five.

\subsection{Experiment 2: Theoretical Analysis of Two-Compartment LIF Neurons}
\label{sec:two_comp_lif_experiment_2}

Given the encouraging results from the previous subsection, we would now like to analyse the theoretical advantage of the two-compartment \LIF model in a more systematic manner.
In particular, our goal is to characterise the \enquote{computational power} of different network and neuron types using the methodology from our basis-function experiment in \Cref{sec:dendritic_computation_theory_numerical}.

We implicitly assume that \Hcond accurately describes the somatic current flowing into the two-compartment neuron.
This is supported by our earlier results from Experiment~1 (\Cref{sec:two_comp_lif_experiment_1}).
Furthermore, note that we abbreviate the two-compartment LIF nonlinearity with conductance-based synapses as \Hcond, and the standard current-based \LIF neuron as \Hcur.
As mentioned before $\Hcur$, is simply defined as $\Hcur(J_\mathrm{E}, J_\mathrm{I}) = J_\mathrm{E} - J_\mathrm{I}$.

\subsubsection{Methods}
Just as in \Cref{sec:dendritic_computation_theory_numerical}, we would like to measure how well different networks can approximate functions of increasing complexity \slc.
We sample random 2D target-current functions $J_\rho$ over $(x_1, x_2) \in [-1, 1]^2$ with an RMS of \SI{0.5}{\nano\ampere} on a $63 \times 63$ grid using the technique illustrated in \Cref{fig:2d_functions_overview}.
We then measure how well $J_\rho$ can be approximated for a single post-neuron given the static tuning of the networks depicted in \Cref{fig:nef_multivariate_functions}.
The pre-populations consist of \num{100} neurons each and project both excitatorily and inhibitorily onto the post-neuron; the intermediate population in the two-layer network (\Cref{fig:nef_multivariate_functions_b}) consists of \num{200} neurons.
In each trial, the pre-population tuning is randomly generated.

All synaptic weights are computed by solving the QP in \cref{eqn:two_comp_optimal_weights_prime_matrix} for 256 randomly selected training samples, with and without subthreshold relaxation (the relaxation threshold is set to \SI{0}{\nano\ampere}).
The regularisation parameters were selected independently for each setup (cf.~\Cref{app:two_comp_regularisation_factor_sweep}).
The final error $E_\mathrm{model}$ is the \NRMSE over all grid points with Gaussian noise (standard deviation $10^{-2} a_\mathrm{max}$) added to the pre-activities to test for generalisation.

\subsubsection{Results}

\begin{figure}
	\centering
	\includegraphics{media/chapters/03_nlif/03_04/two_comp_2d_frequency_sweep.pdf}%
	\kern-158.06mm\includegraphics{media/chapters/03_nlif/03_04/two_comp_2d_frequency_sweep_overlay.pdf}
	\caption[Two-compartment neuron decoding error for random multivariate current functions]{Decoding error for random multivariate current functions.
	\NRMSE between random, two-dimensional current functions and the decoded approximation using different input-dependent nonlinearities; errors are based on the superthreshold error function $\mathcal{E}$ (eq.~\ref{eqn:decode_current_subthreshold}). The low-pass filter coefficient \slc is a proxy for the spatial frequency content in the target function (cf.~\Cref{sec:dendritic_computation_theory_numerical}). All points correspond to the median over \num{100} trials. Dashed lines show results for not taking the subthreshold relaxation into account when solving for weights.
	Dotted lines are theoretical predictions from \Cref{fig:dendritic_computation_fourier_example_b}.	
	The black lines show the results for a linear, current-based \Hcur; blue/green lines show the results for the two-compartment conductance-based model  \Hcond with parameters given in \Cref{tbl:two_comp_model_parameters} (without noise). Orange lines correspond a current-based network with two-dimensional pre-neuron tuning (i.e., a two-layer neural network). Shaded areas correspond to the 25/75 percentile for the current-based models and the conductance-based model with $c_{12} = \SI{50}{\nano\siemens}$.
	}
	\label{fig:two_comp_lif_frequency_sweep}
\end{figure}

Overall, the results depicted in \Cref{fig:two_comp_lif_frequency_sweep} are similar to those from our theoretical basis-function experiment \Cref{sec:nlif_theory} (cf.~\Cref{fig:dendritic_computation_fourier_example}).
In particular, the error curve for the two-compartment neuron qualitatively resembles the multiplicative nonlinearity, although errors tend to be higher for \Hcond:
the inflection point of the sigmoid is at $\slc \approx 0.7$ for the \twocomplif nonlinearity, and was at $\slc \approx 1$ for the multiplicative nonlinearity.

Similarly to additive network in the basis-function experiment (cf.~the dotted line labelled \enquote{additive baseline}), the median error for \Hcur initially increases almost linearly on a log-log plot, starting from a $2.5\%$ error for low-frequency functions to an error of about $50\%$ for functions with \slc greater than one.
Subthreshold relaxation reduces this error by up to $50\%$.

The error for the conductance-based \twocomplif nonlinearity initially increases sub-linearly on a log-log plot, starting at median errors of about $0.8\%$ for low-frequency functions.
The maximum reduction in the median error compared to the standard-\LIF neuron is about $65\%$.
Errors are competitive with the two-layer network for $\slc < 0.5$.
The error converges to the results for the standard \LIF neuron for $\slc > 1$.
Compared to standard \LIF neurons, the advantage of subthreshold relaxation is, on average, not as pronounced.

\subsubsection{Discussion}
The large errors for \Hcur and \Hcond in regimes with $\slc > 0.5$ are likely due to both functions not being able to solve the \XOR problem (cf.~\Cref{app:thm_two_comp_xor}).
Functions with $\slc \geq 0.5$ may possess multiple maxima/minima over $[-1, 1]^2$ (cf.~\Cref{fig:2d_functions_overview}), akin to \XOR.
Trying to approximate these functions using \Hcur or \Hcond thus leads to a large error.

This observation explains the difference between the multiplicative nonlinearity from the basis-function experiment (see the \enquote{multiplicative baseline} in the above figure) and the two-compartment \LIF neuron---the multiplicative nonlinearity \emph{can} be used to solve the \XOR problem.
Evidently, networks capable of approximating \XOR-like functions are, using our terminology form \Cref{sec:dendritic_computation_theory}, more \enquote{powerful} than those limited to conductance-based shunting.

To summarise, this experiment demonstrates that the \twocomplif dendritic nonlinearity \Hcond substantially reduces the approximation error for $J_\rho$ with $\slc < 1$ compared to an additive network with nonlinearity \Hcur.
The two-compartment neuron is competitive with a two-layer network for $\slc < 0.5$ and reaches similar errors as the multiplicative nonlinearity.


\subsection{Experiment 3: Dendritic Computation in Spiking Neural Networks}
\label{sec:two_comp_lif_experiment_3}

Our results for Experiment~1 suggest that we can use the \twocomplif nonlinearity \Hcond to predict the average current flowing into the somatic compartment of a two-compartment neuron.
Furthermore, Experiment~2 shows that we can use \Hcond to approximate a relatively large class of functions well.
In our final experiment, we validate whether these results still hold true in a spiking neural network context.

While we use the same basic network setup as in Experiment~2, we now decode the represented value from a population of \num{100} neurons through time.
Optimally, this decoded value should be $f(x_1, x_2)$, where the input signals $x_1(t), x_2(t) \in [-1, 1]$ are represented by the pre-populations.
Additionally, we now adhere to Dale's principle.
Neurons are randomly marked as either excitatory or inhibitory, with a 30\% probability of a neuron being inhibitory.

We model fast excitatory synapses as an exponential low-pass (cf.~\Cref{sec:simplified_neuron_models}) with a time-constant of \SI{5}{\milli\second} as observed in glutamatergic pyramidal neurons with AMPA receptor \citep{jonas1993quantal}.
The inhibitory pathway is modeled with a \SI{10}{\milli\second} time-constant as observed in inhibitory interneurons with GABA\textsubscript{A} receptors \citep{gupta2000organizing}.
All remaining neuron parameters can be found in \Cref{tbl:two_comp_neuron_parameters}.


\begin{figure}
	\includegraphics{media/chapters/03_nlif/03_04/two_comp_network_spikes_example.pdf}
	\caption[Computing nonnegative multiplication in a spiking neural network using two-compartment LIF neurons]{Computing nonnegative multiplication in a spiking neural network using two-compartment LIF neurons.
	\textbf{(A)} \emph{Top two plots:} inputs $x_1(t)$ and $x_2(t)$ as represented by the two pre-populations. The input is a fourth order 2D Hilbert curve. \emph{Bottom:} mathematical target $f(x_1(t), x_2(t)) = (x_1(t) + 1) (x_2(t) + 1) / 4$, filtered target function, as well as the decoded target population output.
	\textbf{(B)} Spike raster plots corresponding to the spiking activity of each of the populations (only half of the neurons are depicted). Red shaded background corresponds to inhibitory neurons in the pre-populations, all other neurons are excitatory.}
	\label{fig:two_comp_lif_spiking_example}
\end{figure}

In each trial, we simulate the network over \SI{10}{\second} at a time-resolution of \SI{100}{\micro\second}.
Inputs $x_1(t)$ and $x_2(t)$ are moving along a fourth-order space-filling Hilbert curve covering $[-1, 1]^2$ \citep{hilbert1891uber}.
The activities of the output population are decoded and filtered with a first-order exponential low-pass at $\tau = \SI{100}{\milli\second}$.
To determine the approximation error, we pass the target $f(x_1(t), x_2(t))$ through exactly the same filter chain.
Our final error $E_\mathrm{net}$ is the \NRMSE between the decoded output and the filtered target signal over time.
\Cref{fig:two_comp_lif_spiking_example} depicts a single trial.

Again, all synaptic weights are computed by solving the \QP in \cref{eqn:two_comp_optimal_weights_prime_matrix}.
We set the relaxation threshold for subthreshold relaxation to $75\%$ of $J_\mathrm{th}$, as suggested by our experiments in \Cref{sec:nef_subthreshold}.
The regularisation factor $\sigma$ has been selected independently for each parameter set such that $E_\mathrm{net}$ is minimised when computing nonnegative multiplication (cf.~\Cref{app:two_comp_regularisation_factor_sweep}).

\subsubsection{Experiment 3.1: Random bandlimited functions}

\begin{figure}[t]
	\centering
	{\includegraphics{media/chapters/03_nlif/03_04/two_comp_2d_frequency_sweep_network.pdf}}%
	\kern-158.06mm\includegraphics{media/chapters/03_nlif/03_04/two_comp_2d_frequency_sweep_overlay.pdf}
	\caption[Median error for computing random bandlimited functions in a feed-forward network over 100 trials]{Median error for computing random bandlimited functions in a feed-forward network over 100 trials. Measured \NRMSE is the difference between the represented value $\hat y(t)$ and the expected value $f_\rho(x_1(t), x_2(t))$ relative to the standard deviation of $f_\rho$. See \Cref{fig:two_comp_lif_frequency_sweep} for a more detailed description.}
	\label{fig:frequency_sweep_network}
\end{figure}

We first test our setup with the bandlimited functions $f_\rho$ used in Experiment~2.
Results are depicted in \Cref{fig:frequency_sweep_network}.
Qualitatively, the results are similar to what we saw before, although the minimum errors are substantially higher.
The reduction in error between the current- and conductance-based models is not quite as large as suggested by the previous experiment, with a maximum reduction (in terms of the median) of only $40\%$ (instead of $65\%$ before).
While subthreshold relaxation mostly increases the performance of the current-based model in the previous experiment, the improvement in error is now clearly visible for the conductance-based model as well (especially for $c_{12} = \SI{50}{\nano\siemens}$).

Notably, the minimum median approximation error of the two-layer network is about $10\%$, whereas the single-layer current- and conductance-based models reach minimum errors of about $6\%$ and $4\%$, respectively. The two-layer network clearly surpasses the performance of the two-compartment \LIF single-layer network for $\slc > 0.6$.

\paragraph{Discussion}

The elevated error levels are likely due to the spike noise that is now superimposed onto all input signals.
Notably, this noise, as well as the added dynamics, do not affect the two-compartment neuron over-proportionally, as one might expect due to the simplifications made during the derivation of the dendritic nonlinearity model (cf.~\Cref{sec:nlif_derive_h}).
To the contrary, the largest effect of the additional noise is on the two-layer network.

We think that the larger errors for the two-layer network are caused by the representation of a two-dimensional quantity begin noisier than the representation of the individual scalars in the two pre-populations.
This is in line with our observation from \Cref{sec:dendritic_computation_theory_numerical}.
Given a maximum error, the number of basis functions decodable from a multi-dimensional population is not much larger than the decodable function count for a one-dimensional population.

To solve this issue, we would optimally have to square the number of neurons used in the intermediate layer.
In our case, we would have to use $100^2 = \num{10000}$ instead of \num{200}, which would not really be comparable to the single-layer setups.

\subsubsection{Experiment 3.2: Benchmark functions}

\begin{table}[t]
	\caption[Spiking neural network approximation errors.]{Spiking neural network approximation errors for function approximations on $[0, 1]^2$. Error values correspond to the \NRMSE and are measured as the difference between the output decoded from the target population and the desired output for a ten second sweep across a 4th order 2D Hilbert curve\index{Hilbert curve} over the input space. Results are the mean and standard deviation over 256 trials. The best result for a target function is set in bold; darker background colours indicate a worse ranking of the result in the corresponding row. Columns labeled \enquote{standard} refer to the default, single layer network setup, \enquote{two layers} refers to the two-layer setup, and \enquote{noisy} to the single layer network setup with model parameters derived under noise (see Experiment 1.2). Additional tables can be found in \Cref{app:data_chp3}.}
	\fontsize{10pt}{12pt}\selectfont
	\renewcommand\arraystretch{1.2}
	\sffamily
	%--------------------------------------------
	\begin{tabular}{r r r r r r r r }
	\toprule
	\textbf{Target}& \multicolumn{7}{c}{\textbf{Experiment setup}} \\
	\cmidrule(r){1-1}\cmidrule(l){2-8}
	& \multicolumn{3}{c}{%0
	LIF}
	& \multicolumn{2}{c}{%1
	Two comp. $c_{12} = \SI{50}{\nano\siemens}$}
	& \multicolumn{2}{c}{%2
	Two comp. $c_{12} = \SI{100}{\nano\siemens}$}
	\\
	\cmidrule(l){2-4}
	\cmidrule(l){5-6}
	\cmidrule(l){7-8}
	& standard
	& standard\textsuperscript{\dag}
	& two layers\textsuperscript{\dag}
	& %0
	standard\textsuperscript{\dag}
	& %1
	noisy\textsuperscript{\dag}
	& %0
	standard\textsuperscript{\dag}
	& %1
	noisy\textsuperscript{\dag}
	\\
	\midrule
	$x_1 + x_2$ 
	& \cellcolor{White!72!SteelBlue}$5.7 \pm 0.2\%$
	& \cellcolor{White!58!SteelBlue}$5.8 \pm 0.3\%$
	& \cellcolor{White!29!SteelBlue}$9.9 \pm 0.4\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{4.3 \pm 0.3\%}$
	& \cellcolor{White!43!SteelBlue}$8.2 \pm 0.9\%$
	& \cellcolor{White!86!SteelBlue}$4.7 \pm 0.3\%$
	& \cellcolor{White!15!SteelBlue}$11.2 \pm 0.9\%$
	\\
	$x_1 \times x_2$ 
	& \cellcolor{White!15!SteelBlue}$25.8 \pm 1.2\%$
	& \cellcolor{White!29!SteelBlue}$15.5 \pm 1.2\%$
	& \cellcolor{White!43!SteelBlue}$9.2 \pm 0.3\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{4.4 \pm 0.6\%}$
	& \cellcolor{White!86!SteelBlue}$4.7 \pm 0.4\%$
	& \cellcolor{White!72!SteelBlue}$5.4 \pm 0.6\%$
	& \cellcolor{White!58!SteelBlue}$6.9 \pm 0.4\%$
	\\
	$\sqrt{x_1 \times x_2}$ 
	& \cellcolor{White!15!SteelBlue}$24.9 \pm 1.8\%$
	& \cellcolor{White!29!SteelBlue}$14.7 \pm 1.7\%$
	& \cellcolor{White!43!SteelBlue}$12.6 \pm 0.9\%$
	& \cellcolor{White!86!SteelBlue}$7.1 \pm 0.9\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{6.0 \pm 0.7\%}$
	& \cellcolor{White!58!SteelBlue}$7.7 \pm 1.0\%$
	& \cellcolor{White!72!SteelBlue}$7.6 \pm 0.6\%$
	\\
	$(x_1 \times x_2) ^ 2$ 
	& \cellcolor{White!15!SteelBlue}$23.1 \pm 0.7\%$
	& \cellcolor{White!29!SteelBlue}$16.2 \pm 0.9\%$
	& \cellcolor{White!43!SteelBlue}$9.3 \pm 0.5\%$
	& \cellcolor{White!86!SteelBlue}$5.1 \pm 0.9\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{3.9 \pm 0.6\%}$
	& \cellcolor{White!72!SteelBlue}$7.2 \pm 1.2\%$
	& \cellcolor{White!58!SteelBlue}$8.8 \pm 1.0\%$
	\\
	$x_1 / (1 + x_2)$ 
	& \cellcolor{White!15!SteelBlue}$15.0 \pm 0.7\%$
	& \cellcolor{White!58!SteelBlue}$10.1 \pm 0.8\%$
	& \cellcolor{White!72!SteelBlue}$9.2 \pm 0.6\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{4.5 \pm 0.4\%}$
	& \cellcolor{White!43!SteelBlue}$11.4 \pm 1.7\%$
	& \cellcolor{White!86!SteelBlue}$6.1 \pm 0.6\%$
	& \cellcolor{White!29!SteelBlue}$14.8 \pm 1.8\%$
	\\
	$\|(x_1, x_2)\|$ 
	& \cellcolor{White!15!SteelBlue}$18.6 \pm 0.6\%$
	& \cellcolor{White!43!SteelBlue}$10.8 \pm 0.9\%$
	& \cellcolor{White!58!SteelBlue}$9.7 \pm 0.4\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{5.3 \pm 0.5\%}$
	& \cellcolor{White!72!SteelBlue}$8.2 \pm 0.9\%$
	& \cellcolor{White!86!SteelBlue}$6.0 \pm 0.6\%$
	& \cellcolor{White!29!SteelBlue}$12.1 \pm 0.9\%$
	\\
	$\mathrm{atan}(x_1, x_2)$ 
	& \cellcolor{White!15!SteelBlue}$17.8 \pm 1.0\%$
	& \cellcolor{White!43!SteelBlue}$12.4 \pm 1.0\%$
	& \cellcolor{White!29!SteelBlue}$12.5 \pm 0.8\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{7.2 \pm 1.8\%}$
	& \cellcolor{White!86!SteelBlue}$7.5 \pm 1.5\%$
	& \cellcolor{White!58!SteelBlue}$8.3 \pm 1.8\%$
	& \cellcolor{White!72!SteelBlue}$8.1 \pm 1.0\%$
	\\
	$\max(x_1, x_2)$ 
	& \cellcolor{White!15!SteelBlue}$34.5 \pm 1.1\%$
	& \cellcolor{White!29!SteelBlue}$22.6 \pm 1.4\%$
	& \cellcolor{White!86!SteelBlue}$10.0 \pm 0.4\%$
	& \cellcolor{White!72!SteelBlue}$12.9 \pm 1.5\%$
	& \cellcolor{White!100!SteelBlue}$\mathbf{9.9 \pm 1.3\%}$
	& \cellcolor{White!58!SteelBlue}$14.9 \pm 1.8\%$
	& \cellcolor{White!43!SteelBlue}$16.3 \pm 0.9\%$
	\\
	\bottomrule
	\end{tabular}\\[0.25cm]
	%--------------------------------------------
	\raggedright\textsuperscript{\dag}With subthreshold relaxation
	\label{tbl:function_approximations}
\end{table}

While the random functions in the above experiments are useful to systematically characterise the individual setups, it is hard to tell from these data alone what the impact of using two-compartment neurons could be in practice.
To this end, we selected eight benchmark functions $f(x_1, x_2)$ typically found in models of neurobiological systems and repeated the experiment.
See \Cref{tbl:two_comp_functions} for a detailed list.

Note that we rescale the input such that $(x_1, x_2) \in [-1, 1]^2$ map onto $[0, 1]^2$ and $[-1, 1]$.
Similarly, we rescale the output of each function to fully cover the codomain $[-1, 1]$.
This ensures that we fully utilise the range of values that can be represented by the neuron populations, while restricting functions such as multiplication to a single quadrant.

\paragraph{Results}
A summary of the results over $256$ trials per function and setup is given in \Cref{tbl:function_approximations}.
More detailed results can be found in \Cref{tbl:function_approximations_complete}.
The two-compartment model with a coupling conductance of $g_\mathrm{C} = \SI{50}{\nano\second}$ achieves the smallest error $E_\mathrm{net}$ across all target functions.

Using the dendritic nonlinearity model parameters derived under noise (cf.~\Cref{sec:two_comp_lif_experiment_1}) is beneficial when computing multiplicative functions and the maximum.
For these target functions, the synaptic connection matrix tends to be sparser, increasing individual parameter weights and thus input noise (cf.~\Cref{fig:two_comp_weights_examples_statistics_c,fig:two_comp_weights_examples_statistics_d}).
%This increase in noise matches the environment the neuron parameters have been optimised for.

The minimum error for the two-layer network is about $9\%$ even for simple functions, matching the observation we made in the random function experiment above.
The current-based neuron is only competetive with the two-compartment neuron for computing addition.
Subthreshold relaxation decreases the error by up to $40\%$.

\paragraph{Discussion}
An effect that could contribute to the superior performance of the two-compartment neurons are the low-pass filter dynamics of the dendritic compartment.
This filter reduces high-frequency spike noise and thus may positively impact $E_\mathrm{net}$.
We control for this effect in an experiment described in \Cref{app:two_comp_regularisation_factor_sweep}, where we add an optimal low-pass filter to each network setup.
Results are shown in \Cref{tbl:function_approximations_pre_filter}.
A matched pre-filter reduces the error in all setups by only $1\%$-$2\%$, which indicates that the low-pass filter dynamics of the dendritic compartment cannot bethe primary source for the reduction in error.

To summarise our experiments, we demonstrated in three stages (validation of the nonlinearity model \Hcond for a single neuron, purley mathematical properties of \Hcond, and, finally, performance on a network-level) that we are able to successfully incorporate two-compartment LIF neurons, an admittedly simple model of shunting in dendritic trees, into functional modeling frameworks.
Instead of reducing the accuracy of our networks, the added detail can be systematically leveraged for computation.

Our experiments also suggest that---at least in a biologically plausible setting, i.e., using spiking neurons---this type of computation may result in a higher accuracy compared to two-layer architectures that suffer from an increase in the amount of spike-induced temporal noise due to the additional neuron layer.

% TODO: Note that we do not train the encoders!
