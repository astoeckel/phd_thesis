% !TeX spellcheck = en_GB

\section{Levels of Biological Detail}

To demonstrate our approach of adding biological detail, we first focus on a model of temporal basis function generation in the Granule-Golgi circuit discussed above.
We present five models of increasing complexity---the first model is merely an abstract implementation of \cref{eqn:delay_network_lti}, the final model respects spatial sparsity, convergence, tuning curves, and, to a degree, neurotransmitter constraints.
All models are depicted in Fig.~\ref{fig:network_types}.
In all cases, the scalar input $u$ is received from one hundred spiking Leaky Integrate-and-Fire (LIF) neurons with randomly chosen tuning curves, representing input provided by the PCN (see Model B for details).

\paragraph{Model A: \enquote{Direct} Implementation} 
For this model, we directly solve the differential equation in \cref{eqn:delay_network_lti} by integration.
That is, we have a single layer of \enquote{neurons} that are pure integrators (i.e., no non-linearity). The matrix $\mat{A}$ describes the recurrent connection weights, and $\mat{B}$ the input connection weights.
This model does not distinguish between the granule and Golgi cells, and does not include details such as individual neurons or spikes.
Instead, it focuses on the high-level theory of what the system is computing.

\paragraph{Model B: Single Population}
We replace the integrators with a single layer of 200 spiking Leaky Integrate-and-Fire (LIF) neurons.
These neurons form a distributed representation of $\vec{m}$ using a population code.
Each neuron $i$ is parametrized by a randomly chosen preferred stimulus vector $\vec{e}_i$ (for \textit{encoder}), gain $\alpha_i$ and bias current $J^\mathrm{bias}_i$, resulting in a desired response (i.e., tuning curve) for each neuron:
\begin{align}
    a_i(\vec m) = G[J_i(\vec m)] = G[\alpha_i (\vec{e}_i \cdot \vec{m} ) + J^\mathrm{bias}_i] \,,
    \label{eqn:lif_tuning_curve}
\end{align}
where $G$ the is neural response curve of the LIF neuron model.
The parameters $\alpha$ and $J_\mathrm{bias}$ are randomly chosen from a distribution that ensures a maximum firing rate of \SIrange{50}{100}{\hertz}, consistent with biological recordings of granule cells \cite{chadderton2004integration}.
We then use least-squares to solve for optimal input and recurrent connection weights that result in these desired tuning curves while implementing the equivalent calculation as in Model A.
Importantly, when solving for the recurrent connection weights, we also take into account the synaptic filter, which we model here as a decaying exponential (i.e., a low-pass).
This is the standard process in the NEF \cite{eliasmith2003neural}.

\paragraph{Model C: Inter-neurons}
As a next step, we separate the single layer of neurons into two populations corresponding to the Golgi and granule cells, reflecting the actual biology of the cerebellum (see above). This introduces two synaptic filters which need to be taken into account when solving for the connection weights that best approximate \cref{eqn:delay_network_lti}.
Furthermore, to at least approximate the fact that there are far fewer Golgi cells than granule cells, we use 20 Golgi cells and 200 granule cells.

\paragraph{Model D: Inhibition and Excitation}
So far, we have not accounted for Dale's principle, i.e., Golgi cells being purely inhibitory, and granule cells being purely excitatory.
We handle this by switching to the non-negative least-squares problem described in \citet{stoeckel2021passive}. For each post-neuron $i$ we minimize
\begin{align*}
    \min_{\vec w_i^+, \vec w_i^-} \sum_{k = 1}^N \big( \vec w_i^+ \cdot \vec a^+_k - \vec w_i^- \cdot \vec a^-_k - J_i(\vec m_k) \big)^2 \; \text{ w.r.t.} \; \vec w_i^+, \vec w_i^- \geq 0 \,,
\end{align*}
where $\vec a^+_k$, $\vec a^-_k$ are the excitatory and inhibitory pre-activities for sample $k$, $\vec w^+$, $\vec w^-$ are the connection weights for excitatory and inhibitory pre-neurons, and $J_i(\vec m_k)$ is the current required to represent the desired value $\vec m_k$ as defined in \cref{eqn:lif_tuning_curve}.

\begin{figure}[t]
    \centering
    \includegraphics{media/chapters/05_cerebellum/spatial_constraints.pdf}
    \caption[Spatial connectivity constraints.]{Spatial connectivity constraints. \textbf{(A)} Normalized connection probabilities $p_{ij}$ for $\sigma=0.25$. \textbf{(B)} Spatial organisation of the Golgi and granule cells. The background depicts the cumulative density of the Golgi to granule connection probability for a virtual Granule cell at each location (same colours as in \textbf{A}).}
    \label{fig:spatial_constraints}
\end{figure}

\paragraph{Model E: Sparse connectivity and activity}
For this model, we add in realistic constraints on how connected the neurons are.
The previous models used all-to-all connections, whereas for this model, we only allow a subset of those connections to be non-zero.
This applies to both the input to the Granule-Golgi system and for the recurrent connections within the granule cells.  
In particular, we account for the granule cell convergence numbers by randomly selecting five PCN and five Golgi cells as pre-neurons---this number is slightly larger than the number reported above, since, as we discuss below, the number of pre-neurons places a strict upper limit on the connectivity. Given this extremely sparse connectivity, we increase the number of neurons in the simulation to 10\,000 granule and one hundred Golgi cells, which is closer to the ratio observed in nature.

To account for spatially imposed connectivity constraints, we assign a location $\vec x$ in $[-1, 1]^2$ to each neuron. The probability $p_{ij}$ of a post-neuron $i$ to receive a connection from a pre-neuron $j$ is proportional to $\exp \left(- \| \vec x_i - \vec x_j \|^2 / \sigma^{2} \right)$ (Fig.~\ref{fig:spatial_constraints}).

Finally, the input representation in the PC neurons was made more temporally sparse by adjusting the gain and bias parameters in \cref{eqn:lif_tuning_curve}.
Data reported by \citet{chadderton2004integration} indicate that granule cell excitatory event rates---which are driven by PCN activity---drop from an average of \SI{40}{\per\second} with a stimulus being present to only \SI{8.5}{\per\second} when there is no stimulus.
We adjusted the PCN tuning curves to match these statistics in the final network.
